{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"data-protection-notice/","text":"Data Protection Notice The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data. In this data protection policy we inform you about the processing of your personal data when visiting and using our web site. Controller Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Phone: +49 631 20575 0 info@dfki.de Legal Notice Data protection officer Phone: +49 631 20575 0 datenschutz@dfki.de Hosting Server This website is hosted with Github . For more information about github's data processing and contacts, please refer to the Privacy Policy . Access and Intervention Besides the information in this data protection policy you have the right of access to your personal data. To ensure fair data processing, you have the following rights: The right to rectification and completion of your personal data The right to erasure of your personal data The right to restriction of the processing of your personal data The right to object to the processing of your personal data on grounds related to your particular situation To exercise these rights, please contact our data protection officer. Right to lodge a complaint You have the right to lodge a complaint with a supervisory authority if you consider that the processing of your personal data infringes statutory data protection regulations.","title":"Data Protection Notice"},{"location":"data-protection-notice/#data-protection-notice","text":"The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data. In this data protection policy we inform you about the processing of your personal data when visiting and using our web site.","title":"Data Protection Notice"},{"location":"data-protection-notice/#controller","text":"Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Phone: +49 631 20575 0 info@dfki.de Legal Notice","title":"Controller"},{"location":"data-protection-notice/#data-protection-officer","text":"Phone: +49 631 20575 0 datenschutz@dfki.de","title":"Data protection officer"},{"location":"data-protection-notice/#hosting-server","text":"This website is hosted with Github . For more information about github's data processing and contacts, please refer to the Privacy Policy .","title":"Hosting Server"},{"location":"data-protection-notice/#access-and-intervention","text":"Besides the information in this data protection policy you have the right of access to your personal data. To ensure fair data processing, you have the following rights: The right to rectification and completion of your personal data The right to erasure of your personal data The right to restriction of the processing of your personal data The right to object to the processing of your personal data on grounds related to your particular situation To exercise these rights, please contact our data protection officer.","title":"Access and Intervention"},{"location":"data-protection-notice/#right-to-lodge-a-complaint","text":"You have the right to lodge a complaint with a supervisory authority if you consider that the processing of your personal data infringes statutory data protection regulations.","title":"Right to lodge a complaint"},{"location":"legal-notice/","text":"LEGAL NOTICE Responsible service provider Responsible for the content of the domain agri-gaia.github.io/seerep from the point of view of \u00a7 5 TMG: Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Kr\u00fcger Helmut Ditzer Trippstadter Str. 122 67663 Kaiserslautern Germany Phone: +49 631 20575 0 Email: info@dfki.de Register Court: Amtsgericht Kaiserslautern Register Number: HRB 2313 ID-Number: DE 148 646 973 The person responsible for the editorial content of the domain agri-gaia.github.io/seerep of the German Research Center for Artificial Intelligence GmbH within the meaning of \u00a7 18 para. 2 MStV is: Mark Niemeyer Berghoffstra\u00dfe 11 49090 Osnabr\u00fcck Germany Phone: +49 541 386050 2254 E-mail: mark.niemeyer@dfki.de Website URL: www.dfki.de Liability for content As a service provider, Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) is responsible under general law for its own content published on this website in accordance with Section 7 para. 1 of the German Telemedia Act (TMG). DFKI makes every effort to keep the information on our website accurate and current, nevertheless, errors and uncertainties cannot be entirely ruled out. For this reason, DFKI undertakes no liability for ensuring that the provided information is current, accurate or complete, and is not responsible for its quality. DFKI is not liable for material or immaterial damages caused directly or indirectly by the use or non-use of the offered information, or by the use of erroneous and incomplete information, unless willful or grossly negligent fault can be demonstrated. This also applies with respect to software or data provided for download. DFKI reserves the right to modify, expand or delete parts of the website or the entire website without separate announcement, or to cease publication temporarily or definitively. Liability for links Pursuant to Section 7 para. 1 of TMG (German Tele-Media Act), the law limits our responsibility as a service provider to our own content on this website. According to Sections 8 \u2013 10 TMG, we are not obliged to permanently monitor any transmitted or stored third-party information, nor to investigate circumstances that might indicate illegal activity. This does not affect our obligation to remove or block information according to general law. However, we can only assume liability for such data from the point in time at which a concrete legal infringement has been identified. Upon notification of any such legal infringement, we will immediately delete the infringing content. Cross-references (\u201clinks\u201d) to the content providers are to be distinguished from our own content. Our offer includes links to external third-party websites. Providers or operators of linked external pages are always responsible for their respective content. We cannot assume any liability for the content of the linked pages. This third-party content was checked by DFKI when the links were first set up to determine whether any legal infringements existed. At the time of the check, no legal infringements were apparent. However, it cannot be ruled out that the content is subsequently changed by the respective providers. A permanent control of the content of the linked pages is not reasonable without evidence of a legal infringement. Should you believe that the linked external pages infringe applicable law or otherwise contain inappropriate content, please notify us directly at: info@dfki.de. In case DFKI should notice or receive any indication that an external offer to which it has linked might cause civil or criminal liability, DFKI will immediately delete this link. Copyright The layout of the homepage, the graphics used and other content on the DFKI website are protected by copyright. The reproduction, processing, distribution and any type of use outside the boundaries of copyright law require the prior written approval of the DFKI. Insofar as any content on this page was not created by DFKI, the copyrights of third parties will be respected. If you believe that you discovered a copyright infringement, please report this to us accordingly. Upon becoming aware of any legal infringements, DFKI will remove or disable access to such infringing content immediately.","title":"LEGAL NOTICE"},{"location":"legal-notice/#legal-notice","text":"","title":"LEGAL NOTICE"},{"location":"legal-notice/#responsible-service-provider","text":"Responsible for the content of the domain agri-gaia.github.io/seerep from the point of view of \u00a7 5 TMG: Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Kr\u00fcger Helmut Ditzer Trippstadter Str. 122 67663 Kaiserslautern Germany Phone: +49 631 20575 0 Email: info@dfki.de Register Court: Amtsgericht Kaiserslautern Register Number: HRB 2313 ID-Number: DE 148 646 973 The person responsible for the editorial content of the domain agri-gaia.github.io/seerep of the German Research Center for Artificial Intelligence GmbH within the meaning of \u00a7 18 para. 2 MStV is: Mark Niemeyer Berghoffstra\u00dfe 11 49090 Osnabr\u00fcck Germany Phone: +49 541 386050 2254 E-mail: mark.niemeyer@dfki.de Website URL: www.dfki.de","title":"Responsible service provider"},{"location":"legal-notice/#liability-for-content","text":"As a service provider, Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) is responsible under general law for its own content published on this website in accordance with Section 7 para. 1 of the German Telemedia Act (TMG). DFKI makes every effort to keep the information on our website accurate and current, nevertheless, errors and uncertainties cannot be entirely ruled out. For this reason, DFKI undertakes no liability for ensuring that the provided information is current, accurate or complete, and is not responsible for its quality. DFKI is not liable for material or immaterial damages caused directly or indirectly by the use or non-use of the offered information, or by the use of erroneous and incomplete information, unless willful or grossly negligent fault can be demonstrated. This also applies with respect to software or data provided for download. DFKI reserves the right to modify, expand or delete parts of the website or the entire website without separate announcement, or to cease publication temporarily or definitively.","title":"Liability for content"},{"location":"legal-notice/#liability-for-links","text":"Pursuant to Section 7 para. 1 of TMG (German Tele-Media Act), the law limits our responsibility as a service provider to our own content on this website. According to Sections 8 \u2013 10 TMG, we are not obliged to permanently monitor any transmitted or stored third-party information, nor to investigate circumstances that might indicate illegal activity. This does not affect our obligation to remove or block information according to general law. However, we can only assume liability for such data from the point in time at which a concrete legal infringement has been identified. Upon notification of any such legal infringement, we will immediately delete the infringing content. Cross-references (\u201clinks\u201d) to the content providers are to be distinguished from our own content. Our offer includes links to external third-party websites. Providers or operators of linked external pages are always responsible for their respective content. We cannot assume any liability for the content of the linked pages. This third-party content was checked by DFKI when the links were first set up to determine whether any legal infringements existed. At the time of the check, no legal infringements were apparent. However, it cannot be ruled out that the content is subsequently changed by the respective providers. A permanent control of the content of the linked pages is not reasonable without evidence of a legal infringement. Should you believe that the linked external pages infringe applicable law or otherwise contain inappropriate content, please notify us directly at: info@dfki.de. In case DFKI should notice or receive any indication that an external offer to which it has linked might cause civil or criminal liability, DFKI will immediately delete this link.","title":"Liability for links"},{"location":"legal-notice/#copyright","text":"The layout of the homepage, the graphics used and other content on the DFKI website are protected by copyright. The reproduction, processing, distribution and any type of use outside the boundaries of copyright law require the prior written approval of the DFKI. Insofar as any content on this page was not created by DFKI, the copyrights of third parties will be respected. If you believe that you discovered a copyright infringement, please report this to us accordingly. Upon becoming aware of any legal infringements, DFKI will remove or disable access to such infringing content immediately.","title":"Copyright"},{"location":"getting-started/docs/","text":"Docs The documentation is published via GitHub Pages. The responsible workflow builds Doxygen and MkDocs and publishes them on the gh-pages branch. MkDocs focusses on higher level concepts like the installation process and a package overview, while Doxygen is used for code documentation. If you want to work on the documentation locally i.e for a PR follow the steps below. Dependencies If you are not using the SEEREP development container , you need to have doxygen and MkDocs-Material installed, use the commands below: pip3 mkdocs-material sudo apt install doxygen MkDocs To run MkDocs locally switch into the main directory of SEEREP, where the mkdocs.yml is located. Then use mkdocs serve to build and deploy MkDocs on a local http-server. The page should then be available under http://127.0.0.1:8000/ . Doxygen To create the Doxygen output locally switch into the main directory of SEEREP, where the Doxyfile is located and run doxygen Doxyfile . Now an doxygen/html/ folder should be in the same directory, switch into it. If you are not working in the development container you can simply open the index.html with your browser of choice (e.g. firefox index.html ). Otherwise, use python3 -m http.server to start a local web server which serves the content. The page should be available under the default http://0.0.0.0:8000/ address. If you want to run MkDocs and Doxygen at the same time you need to provide a different port to the Doxygen http-server, use python3 -m http.server 8002 instead.","title":"Docs"},{"location":"getting-started/docs/#docs","text":"The documentation is published via GitHub Pages. The responsible workflow builds Doxygen and MkDocs and publishes them on the gh-pages branch. MkDocs focusses on higher level concepts like the installation process and a package overview, while Doxygen is used for code documentation. If you want to work on the documentation locally i.e for a PR follow the steps below.","title":"Docs"},{"location":"getting-started/docs/#dependencies","text":"If you are not using the SEEREP development container , you need to have doxygen and MkDocs-Material installed, use the commands below: pip3 mkdocs-material sudo apt install doxygen","title":"Dependencies"},{"location":"getting-started/docs/#mkdocs","text":"To run MkDocs locally switch into the main directory of SEEREP, where the mkdocs.yml is located. Then use mkdocs serve to build and deploy MkDocs on a local http-server. The page should then be available under http://127.0.0.1:8000/ .","title":"MkDocs"},{"location":"getting-started/docs/#doxygen","text":"To create the Doxygen output locally switch into the main directory of SEEREP, where the Doxyfile is located and run doxygen Doxyfile . Now an doxygen/html/ folder should be in the same directory, switch into it. If you are not working in the development container you can simply open the index.html with your browser of choice (e.g. firefox index.html ). Otherwise, use python3 -m http.server to start a local web server which serves the content. The page should be available under the default http://0.0.0.0:8000/ address. If you want to run MkDocs and Doxygen at the same time you need to provide a different port to the Doxygen http-server, use python3 -m http.server 8002 instead.","title":"Doxygen"},{"location":"getting-started/installationDev/","text":"Development Environment Installation This page provides an overview on how to install the SEEREP development environment. VS-Code Development Container The VS-Code Development Container is the easiest and recommended way to develop SEEREP. Requirements Current Version of VS-Code Docker >= 17.12.0 Development Container Setup Clone the SEEREP repository from Github and open it in VS-Code. git clone https://github.com/agri-gaia/seerep cd seerep/ code . Create a sibling folder next to the repo called seerep-data . This folder will be mounted for the data exchange between host and container. Without this folder, the following steps will fail! mkdir ../seerep-data Install the Remote Containers and Docker VS-Code extension with the following commands or via the extensions tab in Vs-Code. code --install-extension ms-vscode-remote.remote-containers code --install-extension ms-azuretools.vscode-docker Press F1 or CTRL + SHIFT + P in VS-Code and enter Remote-Containers: Reopen Folder in Container . The installation process can take a couple of minutes since, the docker image of SEEREP is downloaded and started. Additionally, all necessary VS Code extensions are installed inside the container and Intellisense, pre-commit hooks are set up.VS-Code may ask you to login to GitHub, to get the latest updates from the repository. Credentials The default username and password for the Docker container are: user: docker password: docker . Pre Commit Checks This repository uses pre-commit checks to identify simple issues in the code base. The checks are automatically run before each commit. If you want to run the pre-commit checks during the development of a commit, use pre-commit run -a . Hints To Fix Errors If the setup or the Remote-Containers: Reopen Folder in Container fails, here are a couple of hints on how to fix them. First make sure that the Docker container is not already running, use docker container stop $VSC_SEEREP_CONTAINER_ID , the container ID can be found using docker ps . Additionally, you can delete all the data regarding SEEREP, to get a fresh installation: docker volume rm seerep-vscode-extensions docker volume rm vscode docker rmi ghcr.io/agri-gaia/seerep_base:latest (docker rmi ghcr.io/agri-gaia/seerep_server:latest) docker rmi vsc-seerep-* Manual Installation It is not recommended to install the following dependencies globally. Some of them are really hard to uninstall. If you still want to install SEEREP in this way, follow the steps: Install ROS Noetic with the official documentation Install SEEREPs dependencies: gRPC , Protocol Buffers , Flatbuffers , HighFive . Therefore, please follow the steps in the base Dockerfile . In order to build SEEREP, we recommend the common build tool from ROS, catkin . Follow the next steps to download and build seerep globally on your system. source /opt/ros/noetic/setup.bash mkdir -p seerep_ws/src cd seerep_ws/src git clone https://github.com/agri-gaia/seerep.git cd .. catkin build","title":"Dev Environment"},{"location":"getting-started/installationDev/#development-environment-installation","text":"This page provides an overview on how to install the SEEREP development environment.","title":"Development Environment Installation"},{"location":"getting-started/installationDev/#vs-code-development-container","text":"The VS-Code Development Container is the easiest and recommended way to develop SEEREP.","title":"VS-Code Development Container"},{"location":"getting-started/installationDev/#requirements","text":"Current Version of VS-Code Docker >= 17.12.0","title":"Requirements"},{"location":"getting-started/installationDev/#development-container-setup","text":"Clone the SEEREP repository from Github and open it in VS-Code. git clone https://github.com/agri-gaia/seerep cd seerep/ code . Create a sibling folder next to the repo called seerep-data . This folder will be mounted for the data exchange between host and container. Without this folder, the following steps will fail! mkdir ../seerep-data Install the Remote Containers and Docker VS-Code extension with the following commands or via the extensions tab in Vs-Code. code --install-extension ms-vscode-remote.remote-containers code --install-extension ms-azuretools.vscode-docker Press F1 or CTRL + SHIFT + P in VS-Code and enter Remote-Containers: Reopen Folder in Container . The installation process can take a couple of minutes since, the docker image of SEEREP is downloaded and started. Additionally, all necessary VS Code extensions are installed inside the container and Intellisense, pre-commit hooks are set up.VS-Code may ask you to login to GitHub, to get the latest updates from the repository.","title":"Development Container Setup"},{"location":"getting-started/installationDev/#credentials","text":"The default username and password for the Docker container are: user: docker password: docker .","title":"Credentials"},{"location":"getting-started/installationDev/#pre-commit-checks","text":"This repository uses pre-commit checks to identify simple issues in the code base. The checks are automatically run before each commit. If you want to run the pre-commit checks during the development of a commit, use pre-commit run -a .","title":"Pre Commit Checks"},{"location":"getting-started/installationDev/#hints-to-fix-errors","text":"If the setup or the Remote-Containers: Reopen Folder in Container fails, here are a couple of hints on how to fix them. First make sure that the Docker container is not already running, use docker container stop $VSC_SEEREP_CONTAINER_ID , the container ID can be found using docker ps . Additionally, you can delete all the data regarding SEEREP, to get a fresh installation: docker volume rm seerep-vscode-extensions docker volume rm vscode docker rmi ghcr.io/agri-gaia/seerep_base:latest (docker rmi ghcr.io/agri-gaia/seerep_server:latest) docker rmi vsc-seerep-*","title":"Hints To Fix Errors"},{"location":"getting-started/installationDev/#manual-installation","text":"It is not recommended to install the following dependencies globally. Some of them are really hard to uninstall. If you still want to install SEEREP in this way, follow the steps: Install ROS Noetic with the official documentation Install SEEREPs dependencies: gRPC , Protocol Buffers , Flatbuffers , HighFive . Therefore, please follow the steps in the base Dockerfile . In order to build SEEREP, we recommend the common build tool from ROS, catkin . Follow the next steps to download and build seerep globally on your system. source /opt/ros/noetic/setup.bash mkdir -p seerep_ws/src cd seerep_ws/src git clone https://github.com/agri-gaia/seerep.git cd .. catkin build","title":"Manual Installation"},{"location":"getting-started/kubernetes-deployment/","text":"Kubernetes Deployment Besides, the local installation and the usage of an available docker container (see installation ), one can also deploy the seerep-server within a kubernetes cluster. Relevant files Seerep can either be installed with the latest development state or the latest stable version. The relevant files can be found under /docker/kustomize/base --> development /docker/kustomize/overlays/production --> latest stable release The base-folder contains all yaml-files for a cluster deployments. This includes Deployment PersistentVolume and PersistentVolumeClaim Service Ingress Configuration The yaml-file for the cluster is create via Kustomize , hence this folder also contains a kustomization.yaml which puts everything together. The /overlay/production folder contains a second kustomization.yaml. Within this file everything needed to install a production system is overridden. This means, that the sealed secret is replaced with a new one, while the base secret is deleted. Further, the labels and the names of PV and PVC are changed to create new storage explicitly for the production system. Finally, the used image is replaced with the latest stable release. The usage of an overlay, thereby, follows the principles of Kustomize. Building with Kustomize Before one can build the kubernetes manifest, one needs to install Kustomize ( Kustomize installation ) The easiest way to that is the download the latest binary from the offical release page To install the base-version of seerep either one can run: kustomize build base/ > seerep-deployment.yaml to store the manifest in a separate yaml file. Or directly use kubectl : kubectl apply -k base/ In order to install the production version, the commands look slightly different: kustomize build overlays/production/ > seerep-deployment.yaml kubectl apply -k overlays/production/ If someone has a running ArgoCD instance, it is also possible to integrate seerep as a project into ArgoCD. Sealed Secrets The certificates used for the secured ingress are created as sealed-secret . Hence, the secret can safely be stored in a repository. The sealed secret controller installed within the cluster will take care of unsealing the secret and make it usable. To combine Kustomize and sealed secrets this blogs-post was followed faun.pub","title":"Kubernetes Deployment"},{"location":"getting-started/kubernetes-deployment/#kubernetes-deployment","text":"Besides, the local installation and the usage of an available docker container (see installation ), one can also deploy the seerep-server within a kubernetes cluster.","title":"Kubernetes Deployment"},{"location":"getting-started/kubernetes-deployment/#relevant-files","text":"Seerep can either be installed with the latest development state or the latest stable version. The relevant files can be found under /docker/kustomize/base --> development /docker/kustomize/overlays/production --> latest stable release The base-folder contains all yaml-files for a cluster deployments. This includes Deployment PersistentVolume and PersistentVolumeClaim Service Ingress Configuration The yaml-file for the cluster is create via Kustomize , hence this folder also contains a kustomization.yaml which puts everything together. The /overlay/production folder contains a second kustomization.yaml. Within this file everything needed to install a production system is overridden. This means, that the sealed secret is replaced with a new one, while the base secret is deleted. Further, the labels and the names of PV and PVC are changed to create new storage explicitly for the production system. Finally, the used image is replaced with the latest stable release. The usage of an overlay, thereby, follows the principles of Kustomize.","title":"Relevant files"},{"location":"getting-started/kubernetes-deployment/#building-with-kustomize","text":"Before one can build the kubernetes manifest, one needs to install Kustomize ( Kustomize installation ) The easiest way to that is the download the latest binary from the offical release page To install the base-version of seerep either one can run: kustomize build base/ > seerep-deployment.yaml to store the manifest in a separate yaml file. Or directly use kubectl : kubectl apply -k base/ In order to install the production version, the commands look slightly different: kustomize build overlays/production/ > seerep-deployment.yaml kubectl apply -k overlays/production/ If someone has a running ArgoCD instance, it is also possible to integrate seerep as a project into ArgoCD.","title":"Building with Kustomize"},{"location":"getting-started/kubernetes-deployment/#sealed-secrets","text":"The certificates used for the secured ingress are created as sealed-secret . Hence, the secret can safely be stored in a repository. The sealed secret controller installed within the cluster will take care of unsealing the secret and make it usable. To combine Kustomize and sealed secrets this blogs-post was followed faun.pub","title":"Sealed Secrets"},{"location":"getting-started/local-deployment/","text":"Local Deployment The local deployment is based on the seerep_server docker image . The image with the latest (unstable) version can be pulled with the following command. It is recommended to use a version tag instead of latest . docker pull ghcr.io/agri-gaia/seerep_server:latest docker run Run the following command to start the server using docker run . It is recommended to use a version tag instead of latest . docker run \\ --volume=seerep-data:/mnt/seerep-data \\ --publish=9090:9090 \\ --name=seerep_server \\ --tty \\ ghcr.io/agri-gaia/seerep_server:latest \\ --data-folder=/mnt/seerep-data docker-compose Run docker-compose up in the folder of the docker-compose.yml to start the server. It is recommended to use a version tag instead of latest . For this docker compose has to be installed. In the latest version docker compose without a hyphen as part of the Docker CLI replaces docker-compose . Example docker-compose.yml: version: \"3.6\" services: seerep: image: ghcr.io/agri-gaia/seerep_server:latest tty: true container_name: seerep_server command: # define data-dir for seerep-server - \"--data-folder=/mnt/seerep-data\" ports: # the gRPC port - 9090:9090 volumes: # persist the data folder - seerep-data:/mnt/seerep-data #using docker volume #- /your/local/absolute/path:/mnt/seerep-data #using host folder volumes: seerep-data:","title":"Local Deployment"},{"location":"getting-started/local-deployment/#local-deployment","text":"The local deployment is based on the seerep_server docker image . The image with the latest (unstable) version can be pulled with the following command. It is recommended to use a version tag instead of latest . docker pull ghcr.io/agri-gaia/seerep_server:latest","title":"Local Deployment"},{"location":"getting-started/local-deployment/#docker-run","text":"Run the following command to start the server using docker run . It is recommended to use a version tag instead of latest . docker run \\ --volume=seerep-data:/mnt/seerep-data \\ --publish=9090:9090 \\ --name=seerep_server \\ --tty \\ ghcr.io/agri-gaia/seerep_server:latest \\ --data-folder=/mnt/seerep-data","title":"docker run"},{"location":"getting-started/local-deployment/#docker-compose","text":"Run docker-compose up in the folder of the docker-compose.yml to start the server. It is recommended to use a version tag instead of latest . For this docker compose has to be installed. In the latest version docker compose without a hyphen as part of the Docker CLI replaces docker-compose . Example docker-compose.yml: version: \"3.6\" services: seerep: image: ghcr.io/agri-gaia/seerep_server:latest tty: true container_name: seerep_server command: # define data-dir for seerep-server - \"--data-folder=/mnt/seerep-data\" ports: # the gRPC port - 9090:9090 volumes: # persist the data folder - seerep-data:/mnt/seerep-data #using docker volume #- /your/local/absolute/path:/mnt/seerep-data #using host folder volumes: seerep-data:","title":"docker-compose"},{"location":"getting-started/tests/","text":"Tests We are currently working on integrating more tests into SEEREP. GoogleTest is used as a testing framework. The tests are run in a GitHub workflow for every PR and push to the main branch, the tests can also be run locally in a couple of different ways. Running tests locally Catkin The tests can be run via catkin in the command line. When run without a specific package, all tests in the workspace are executed. But while this is very convenient, catkin does not provide much information/output if a test fails. catkin test ( <specific-package> ) Vs-Code Another way to run the test is via the Vs-Code test explorer (triangle test-tube on the left bar of VS-Code). If you have done a fresh installation of the project, it can happen, that the test cases won't be recognized. In order to fix that, just restart the development container. For that, you can use Reopen Folder Locally and then Reopen In Container again. Now you should be able to see the test cases as, in the example below: The icons in the top of the test explorer are mostly self-explanatory, refresh, all tests can be run, a single test can be debugged, and a terminal can be opened to print the output of the tests. Executables If you would like to run the tests via their executables, they are located under /seerep/devel/bin/<test-name> or /seerep/build/<package>/<test-name> .","title":"Tests"},{"location":"getting-started/tests/#tests","text":"We are currently working on integrating more tests into SEEREP. GoogleTest is used as a testing framework. The tests are run in a GitHub workflow for every PR and push to the main branch, the tests can also be run locally in a couple of different ways.","title":"Tests"},{"location":"getting-started/tests/#running-tests-locally","text":"","title":"Running tests locally"},{"location":"getting-started/tests/#catkin","text":"The tests can be run via catkin in the command line. When run without a specific package, all tests in the workspace are executed. But while this is very convenient, catkin does not provide much information/output if a test fails. catkin test ( <specific-package> )","title":"Catkin"},{"location":"getting-started/tests/#vs-code","text":"Another way to run the test is via the Vs-Code test explorer (triangle test-tube on the left bar of VS-Code). If you have done a fresh installation of the project, it can happen, that the test cases won't be recognized. In order to fix that, just restart the development container. For that, you can use Reopen Folder Locally and then Reopen In Container again. Now you should be able to see the test cases as, in the example below: The icons in the top of the test explorer are mostly self-explanatory, refresh, all tests can be run, a single test can be debugged, and a terminal can be opened to print the output of the tests.","title":"Vs-Code"},{"location":"getting-started/tests/#executables","text":"If you would like to run the tests via their executables, they are located under /seerep/devel/bin/<test-name> or /seerep/build/<package>/<test-name> .","title":"Executables"},{"location":"home/","text":"Home The objective of SEEREP (SEmantic Environment REPresentation) is to store generated robot data and enable fast spatio-temporal-semantic queries over the data. Context Autonomous robotic systems must be aware of their environment, in order to safely achieve their goal-oriented actions. Especially in unstructured and changing environments, a detailed model of the environment is required for reasoning and planning. The sensors of a robot provide spatial information via the robot's pose, temporal information is created by the point in time when a sensor is read. Semantic information always exists implicitly and can be made explicit by algorithms or manual labeling. Most existing environment representations focus on one or two of these information types, SEEREP is able to store all three. Thereby, SEEREP enables the robot to reason on a higher level and disambiguate sensor data based on the context. Core Features Fast spatio-temporal-semantic queries with gRPC . Storage of data generated by the robotic system: Offline on the robot (no or slow internet connection), currently in Progress #89 Online on a server-cluster, with gRPC . Shifting computation loads away from the robot and into the cloud. Easily switch between Protocol Buffers (PB) / Flatbuffers (FB) as the messaging format. Architecture The following graphic provides a broad overview of SEEREPs components. The sensor data along with results from processing the data and annotations are stored in HDF5 files. SEEREP uses projects to group common information (e.g. a scanning campaign). The data can be saved locally on the robot or sent into the cloud with gRPC. Therefore, the computational load on the robot can be reduced and algorithms can fetch the data subset which they actually need. A more detailed version, with all ROS packages and message types is available in the package overview .","title":"Home"},{"location":"home/#home","text":"The objective of SEEREP (SEmantic Environment REPresentation) is to store generated robot data and enable fast spatio-temporal-semantic queries over the data.","title":"Home"},{"location":"home/#context","text":"Autonomous robotic systems must be aware of their environment, in order to safely achieve their goal-oriented actions. Especially in unstructured and changing environments, a detailed model of the environment is required for reasoning and planning. The sensors of a robot provide spatial information via the robot's pose, temporal information is created by the point in time when a sensor is read. Semantic information always exists implicitly and can be made explicit by algorithms or manual labeling. Most existing environment representations focus on one or two of these information types, SEEREP is able to store all three. Thereby, SEEREP enables the robot to reason on a higher level and disambiguate sensor data based on the context.","title":"Context"},{"location":"home/#core-features","text":"Fast spatio-temporal-semantic queries with gRPC . Storage of data generated by the robotic system: Offline on the robot (no or slow internet connection), currently in Progress #89 Online on a server-cluster, with gRPC . Shifting computation loads away from the robot and into the cloud. Easily switch between Protocol Buffers (PB) / Flatbuffers (FB) as the messaging format.","title":"Core Features"},{"location":"home/#architecture","text":"The following graphic provides a broad overview of SEEREPs components. The sensor data along with results from processing the data and annotations are stored in HDF5 files. SEEREP uses projects to group common information (e.g. a scanning campaign). The data can be saved locally on the robot or sent into the cloud with gRPC. Therefore, the computational load on the robot can be reduced and algorithms can fetch the data subset which they actually need. A more detailed version, with all ROS packages and message types is available in the package overview .","title":"Architecture"},{"location":"reference/packages/","text":"Package Overview This page provides an overview of all the ROS-Packages used in SEEREP. General SEEREP Structure The general structure of SEEREP is schematically illustrated in the following graphic. SEEREP can be split into two parts, one of which runs on the robot (left box) and one which runs on a server clusters (right box). The communication is handled via gRPC and protocol-buffers (PB) or flatbuffers (FB) . Packages In the following, each package will be described in more detail. seerep-hdf5 The seerep-hdf5 unit provides access to the hdf5 files to store or retrieve data. The unit is split into three packages seerep-hdf5-core , seerep-hdf5-pb and seerep-hdf5-fb . This is to have a server-core which is independent of the message format, so that it's possible to easily switch between PB and FB or any other message format. The main task for the seerep-core is to read UUIDs, bounding boxes (BB), time/semantic information on provided indices from the hdf5-files. The only write operation of the core is to create new hdf5-files. Due to the independence of FB and PB, new communication-messages are added to seerep-msgs ( seerep-msgs/core ). seerep-hdf5-pb and seerep-hdf5-fb provide methods to read or write point clouds, images and transformations from PB or FB messages. seerep-srv The seerep-srv is split into four parts seerep-server , seerep-core and seerep-core-pb , seerep-core-fb . The seerep-server provides the top level interface for the SEEREP server cluster, services which clients can be registered here. The server passes request to the corresponding unit in the layer below (see graphic). The seerep-core-pb / seerep-core-fb writes incoming PB / FB messages to the hdf5 files. In case of a query the seerep-core is asked for the UUIDs of the datasets which match the query parameters. seerep-msgs The seerep-msgs package defines all the PB, FB and core messages used in SEEREP. seerep-ros seerep-ros provides three packages which run on the robot itself. The seerep_ros_conversions_pb/fb packages simply convert ROS messages to PB/FB and vice versa. The second package seerep_ros_communication is used to save sensor information like images and point clouds on the robot, or in case of a good internet connection to the remote server-cluster. Further, the robot is able to query the server for information to support his understanding of the environment and aid its navigation. The seerep_ros_communication\\client is responsible for sending sensor information directly to the remote server. The seerep_ros_communication\\querier is used to get information from the remote server. seerep_ros_communication\\hdf5-dump is used to save sensor information on a hard drive which is located on the robot. seerep-com seerep-com is used to define the gRPC services in PB and FB.","title":"Package Overview"},{"location":"reference/packages/#package-overview","text":"This page provides an overview of all the ROS-Packages used in SEEREP.","title":"Package Overview"},{"location":"reference/packages/#general-seerep-structure","text":"The general structure of SEEREP is schematically illustrated in the following graphic. SEEREP can be split into two parts, one of which runs on the robot (left box) and one which runs on a server clusters (right box). The communication is handled via gRPC and protocol-buffers (PB) or flatbuffers (FB) .","title":"General SEEREP Structure"},{"location":"reference/packages/#packages","text":"In the following, each package will be described in more detail.","title":"Packages"},{"location":"reference/packages/#seerep-hdf5","text":"The seerep-hdf5 unit provides access to the hdf5 files to store or retrieve data. The unit is split into three packages seerep-hdf5-core , seerep-hdf5-pb and seerep-hdf5-fb . This is to have a server-core which is independent of the message format, so that it's possible to easily switch between PB and FB or any other message format. The main task for the seerep-core is to read UUIDs, bounding boxes (BB), time/semantic information on provided indices from the hdf5-files. The only write operation of the core is to create new hdf5-files. Due to the independence of FB and PB, new communication-messages are added to seerep-msgs ( seerep-msgs/core ). seerep-hdf5-pb and seerep-hdf5-fb provide methods to read or write point clouds, images and transformations from PB or FB messages.","title":"seerep-hdf5"},{"location":"reference/packages/#seerep-srv","text":"The seerep-srv is split into four parts seerep-server , seerep-core and seerep-core-pb , seerep-core-fb . The seerep-server provides the top level interface for the SEEREP server cluster, services which clients can be registered here. The server passes request to the corresponding unit in the layer below (see graphic). The seerep-core-pb / seerep-core-fb writes incoming PB / FB messages to the hdf5 files. In case of a query the seerep-core is asked for the UUIDs of the datasets which match the query parameters.","title":"seerep-srv"},{"location":"reference/packages/#seerep-msgs","text":"The seerep-msgs package defines all the PB, FB and core messages used in SEEREP.","title":"seerep-msgs"},{"location":"reference/packages/#seerep-ros","text":"seerep-ros provides three packages which run on the robot itself. The seerep_ros_conversions_pb/fb packages simply convert ROS messages to PB/FB and vice versa. The second package seerep_ros_communication is used to save sensor information like images and point clouds on the robot, or in case of a good internet connection to the remote server-cluster. Further, the robot is able to query the server for information to support his understanding of the environment and aid its navigation. The seerep_ros_communication\\client is responsible for sending sensor information directly to the remote server. The seerep_ros_communication\\querier is used to get information from the remote server. seerep_ros_communication\\hdf5-dump is used to save sensor information on a hard drive which is located on the robot.","title":"seerep-ros"},{"location":"reference/packages/#seerep-com","text":"seerep-com is used to define the gRPC services in PB and FB.","title":"seerep-com"},{"location":"tutorials/images/","text":"Sending & Querying images Sending images In this example we want to send images with labeled bounding boxes as well as general labels to SEEREP. Additionally we add some coordinate transformations at the end. Source: examples/python/gRPC/images/gRPC_pb_sendLabeledImage.py #!/usr/bin/env python3 import os import sys import time import uuid import boundingbox2d_labeled_pb2 as bb import image_pb2 as image import image_service_pb2_grpc as imageService import label_with_instance_pb2 as labelWithInstance import meta_operations_pb2_grpc as metaOperations import numpy as np import projectCreation_pb2 as projectCreation import tf_service_pb2_grpc as tfService import transform_stamped_pb2 as tf from google.protobuf import empty_pb2 script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel ( target = \"local\" ) # 1. Get gRPC service objects stub = imageService . ImageServiceStub ( channel ) stubTf = tfService . TfServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) # 2. Get all projects from the server response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # 3. Check if we have an existing test project, if not, one is created. found = False for project in response . projects : print ( project . name + \" \" + project . uuid ) if project . name == \"testproject\" : projectuuid = project . uuid found = True if not found : creation = projectCreation . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" ) projectCreated = stubMeta . CreateProject ( creation ) projectuuid = projectCreated . uuid theTime = int ( time . time ()) # 4. Create ten images for n in range ( 10 ): theImage = image . Image () rgb = [] lim = 256 # 256 x 256 pixels for i in range ( lim ): for j in range ( lim ): x = float ( i ) / lim y = float ( j ) / lim z = float ( j ) / lim r = np . ubyte (( x * 255.0 + n ) % 255 ) g = np . ubyte (( y * 255.0 + n ) % 255 ) b = np . ubyte (( z * 255.0 + n ) % 255 ) rgb . append ( r ) rgb . append ( g ) rgb . append ( b ) # Add image meta-data theImage . header . frame_id = \"camera\" theImage . header . stamp . seconds = theTime + n theImage . header . stamp . nanos = 0 theImage . header . uuid_project = projectuuid theImage . height = lim theImage . width = lim theImage . encoding = \"rgb8\" theImage . step = 3 * lim # Add image data theImage . data = bytes ( rgb ) # 5. Create bounding boxes with labels bb1 = bb . BoundingBox2DLabeled () for i in range ( 0 , 2 ): bb1 . labelWithInstance . label = \"testlabel\" + str ( i ) bb1 . labelWithInstance . instanceUuid = str ( uuid . uuid4 ()) bb1 . boundingBox . point_min . x = 0.01 + i / 10 bb1 . boundingBox . point_min . y = 0.02 + i / 10 bb1 . boundingBox . point_max . x = 0.03 + i / 10 bb1 . boundingBox . point_max . y = 0.04 + i / 10 theImage . labels_bb . append ( bb1 ) # 6. Add general labels to the image for i in range ( 0 , 2 ): label = labelWithInstance . LabelWithInstance () label . label = \"testlabelgeneral\" + str ( i ) # assuming that that the general labels are not instance related -> no instance uuid # label.instanceUuid = str(uuid.uuid4()) theImage . labels_general . append ( label ) # 7. Send image to the server uuidImg = stub . TransferImage ( theImage ) print ( \"uuid of transfered img: \" + uuidImg . message ) # 8. Add coordinate transformations and send them to the server theTf = tf . TransformStamped () theTf . header . frame_id = \"map\" theTf . header . stamp . seconds = theTime theTf . header . uuid_project = projectuuid theTf . child_frame_id = \"camera\" theTf . transform . translation . x = 1 theTf . transform . translation . y = 2 theTf . transform . translation . z = 3 theTf . transform . rotation . x = 0 theTf . transform . rotation . y = 0 theTf . transform . rotation . z = 0 theTf . transform . rotation . w = 1 stubTf . TransferTransformStamped ( theTf ) theTf . header . stamp . seconds = theTime + 10 theTf . transform . translation . x = 100 theTf . transform . translation . y = 200 theTf . transform . translation . z = 300 stubTf . TransferTransformStamped ( theTf ) Output: uuid of transfered img: 5836f989-adbb-46a0-a689-9e0527d457fe uuid of transfered img: 5b39487d-238a-43e4-a8a4-0f7efb876e8b uuid of transfered img: 00ced216-40b1-4d54-817f-11c413b228c6 uuid of transfered img: 5d330208-e534-4bfb-b242-00295e6b027d uuid of transfered img: 18f00f33-0ac5-4221-a428-13e2c37b27cd uuid of transfered img: ed5134c8-da18-476f-9c1b-60cea04c5627 uuid of transfered img: 22c995b0-c2b6-4c94-85a3-b1118ae3ac7d uuid of transfered img: 9d980608-85ff-4e62-a6be-01c05bd59de9 uuid of transfered img: 24204e3f-1cbd-4cab-99b6-803b3944b450 uuid of transfered img: 797f2e1a-f9e8-4ed3-94b3-a5d101aa0697 Query images Now we want to query the previously send images with some criteria. Possible query parameters are: Bounding Boxes (spatial query) A time interval (temporal query) Labels (semantic query) Protocol Buffers Flatbuffers Source: examples/python/gRPC/images/gRPC_pb_queryImage.py #!/usr/bin/env python3 import os import sys import image_service_pb2_grpc as imageService import meta_operations_pb2_grpc as metaOperations import query_pb2 as query from google.protobuf import empty_pb2 # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service objects stub = imageService . ImageServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) # 2. Get all projects from the server response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # 3. Check if we have an existing test project, if not, we stop here projectuuid = \"\" for project in response . projects : print ( project . name + \" \" + project . uuid + \" \\n \" ) if project . name == \"testproject\" : projectuuid = project . uuid if projectuuid == \"\" : sys . exit () # 4. Create a query with parameters theQuery = query . Query () theQuery . projectuuid . append ( projectuuid ) theQuery . boundingboxstamped . header . frame_id = \"map\" theQuery . boundingboxstamped . boundingbox . point_min . x = 0.0 theQuery . boundingboxstamped . boundingbox . point_min . y = 0.0 theQuery . boundingboxstamped . boundingbox . point_min . z = 0.0 theQuery . boundingboxstamped . boundingbox . point_max . x = 100.0 theQuery . boundingboxstamped . boundingbox . point_max . y = 100.0 theQuery . boundingboxstamped . boundingbox . point_max . z = 100.0 # since epoche theQuery . timeinterval . time_min . seconds = 1638549273 theQuery . timeinterval . time_min . nanos = 0 theQuery . timeinterval . time_max . seconds = 1938549273 theQuery . timeinterval . time_max . nanos = 0 # labels theQuery . label . extend ([ \"testlabel0\" ]) # 5. Query the server for images matching the query and iterate over them for img in stub . GetImage ( theQuery ): print ( f \"uuidmsg: { img . header . uuid_msgs } \" ) print ( f \"first label: { img . labels_bb [ 0 ] . labelWithInstance . label } \" ) print ( \"First bounding box (Xmin, Ymin, Xmax, Ymax): \" + str ( img . labels_bb [ 0 ] . boundingBox . point_min . x ) + \" \" + str ( img . labels_bb [ 0 ] . boundingBox . point_min . y ) + \" \" + str ( img . labels_bb [ 0 ] . boundingBox . point_max . x ) + \" \" + str ( img . labels_bb [ 0 ] . boundingBox . point_max . y ) + \" \\n \" ) Output: testproject 3af70ba8-1e81-4f60-86d2-a4257d88f01e first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 Source: examples/images/gRPC/images/gRPC_fb_queryImage.py #!/usr/bin/env python3 import os import sys import flatbuffers from fb import Image from fb import image_service_grpc_fb as imageService # importing util functions. Assuming that these files are in the parent dir # examples/python/gRPC/util.py # examples/python/gRPC/util_fb.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util import util_fb builder = flatbuffers . Builder ( 1024 ) # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get all projects from the server projectuuid = util_fb . getProject ( builder , channel , 'testproject' ) # 2. Check if the defined project exist; if not exit if not projectuuid : exit () # 3. Get gRPC service object stub = imageService . ImageServiceStub ( channel ) # Create all necessary objects for the query header = util_fb . createHeader ( builder , frame = \"map\" ) pointMin = util_fb . createPoint ( builder , 0.0 , 0.0 , 0.0 ) pointMax = util_fb . createPoint ( builder , 100.0 , 100.0 , 100.0 ) boundingboxStamped = util_fb . createBoundingBoxStamped ( builder , header , pointMin , pointMax ) timeMin = util_fb . createTimeStamp ( builder , 1610549273 , 0 ) timeMax = util_fb . createTimeStamp ( builder , 1938549273 , 0 ) timeInterval = util_fb . createTimeInterval ( builder , timeMin , timeMax ) projectUuids = [ builder . CreateString ( projectuuid )] labels = [ builder . CreateString ( \"testlabel0\" )] dataUuids = [ builder . CreateString ( \"3e12e18d-2d53-40bc-a8af-c5cca3c3b248\" )] instanceUuids = [ builder . CreateString ( \"3e12e18d-2d53-40bc-a8af-c5cca3c3b248\" )] # 4. Create a query with parameters # all parameters are optional # with all parameters set (especially with the data and instance uuids set) the result # of the query will be empty. Set the query parameters to adequate values or remove # them from the query creation query = util_fb . createQuery ( builder , # boundingBox=boundingboxStamped, # timeInterval=timeInterval, # labels=labels, # projectUuids=projectUuids, # instanceUuids=instanceUuids, # dataUuids=dataUuids, withoutData = True , ) builder . Finish ( query ) buf = builder . Output () # 5. Query the server for images matching the query and iterate over them for responseBuf in stub . GetImage ( bytes ( buf )): response = Image . Image . GetRootAs ( responseBuf ) print ( f \"uuidmsg: { response . Header () . UuidMsgs () . decode ( 'utf-8' ) } \" ) print ( \"first label: \" + response . LabelsBb ( 0 ) . LabelWithInstance () . Label () . decode ( \"utf-8\" )) print ( \"first bounding box (Xmin,Ymin,Xmax,Ymax): \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMin () . X ()) + \" \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMin () . Y ()) + \" \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMax () . X ()) + \" \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMax () . Y ()) + \" \\n \" ) print ( \"done.\" ) Output: testproject 3af70ba8-1e81-4f60-86d2-a4257d88f01e uuidmsg: 00ced216-40b1-4d54-817f-11c413b228c6 first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 uuidmsg: 5836f989-adbb-46a0-a689-9e0527d457fe first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 uuidmsg: 5b39487d-238a-43e4-a8a4-0f7efb876e8b first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 uuidmsg: 5d330208-e534-4bfb-b242-00295e6b027d first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 done.","title":"Sending & Querying Images"},{"location":"tutorials/images/#sending-querying-images","text":"","title":"Sending &amp; Querying images"},{"location":"tutorials/images/#sending-images","text":"In this example we want to send images with labeled bounding boxes as well as general labels to SEEREP. Additionally we add some coordinate transformations at the end. Source: examples/python/gRPC/images/gRPC_pb_sendLabeledImage.py #!/usr/bin/env python3 import os import sys import time import uuid import boundingbox2d_labeled_pb2 as bb import image_pb2 as image import image_service_pb2_grpc as imageService import label_with_instance_pb2 as labelWithInstance import meta_operations_pb2_grpc as metaOperations import numpy as np import projectCreation_pb2 as projectCreation import tf_service_pb2_grpc as tfService import transform_stamped_pb2 as tf from google.protobuf import empty_pb2 script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel ( target = \"local\" ) # 1. Get gRPC service objects stub = imageService . ImageServiceStub ( channel ) stubTf = tfService . TfServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) # 2. Get all projects from the server response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # 3. Check if we have an existing test project, if not, one is created. found = False for project in response . projects : print ( project . name + \" \" + project . uuid ) if project . name == \"testproject\" : projectuuid = project . uuid found = True if not found : creation = projectCreation . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" ) projectCreated = stubMeta . CreateProject ( creation ) projectuuid = projectCreated . uuid theTime = int ( time . time ()) # 4. Create ten images for n in range ( 10 ): theImage = image . Image () rgb = [] lim = 256 # 256 x 256 pixels for i in range ( lim ): for j in range ( lim ): x = float ( i ) / lim y = float ( j ) / lim z = float ( j ) / lim r = np . ubyte (( x * 255.0 + n ) % 255 ) g = np . ubyte (( y * 255.0 + n ) % 255 ) b = np . ubyte (( z * 255.0 + n ) % 255 ) rgb . append ( r ) rgb . append ( g ) rgb . append ( b ) # Add image meta-data theImage . header . frame_id = \"camera\" theImage . header . stamp . seconds = theTime + n theImage . header . stamp . nanos = 0 theImage . header . uuid_project = projectuuid theImage . height = lim theImage . width = lim theImage . encoding = \"rgb8\" theImage . step = 3 * lim # Add image data theImage . data = bytes ( rgb ) # 5. Create bounding boxes with labels bb1 = bb . BoundingBox2DLabeled () for i in range ( 0 , 2 ): bb1 . labelWithInstance . label = \"testlabel\" + str ( i ) bb1 . labelWithInstance . instanceUuid = str ( uuid . uuid4 ()) bb1 . boundingBox . point_min . x = 0.01 + i / 10 bb1 . boundingBox . point_min . y = 0.02 + i / 10 bb1 . boundingBox . point_max . x = 0.03 + i / 10 bb1 . boundingBox . point_max . y = 0.04 + i / 10 theImage . labels_bb . append ( bb1 ) # 6. Add general labels to the image for i in range ( 0 , 2 ): label = labelWithInstance . LabelWithInstance () label . label = \"testlabelgeneral\" + str ( i ) # assuming that that the general labels are not instance related -> no instance uuid # label.instanceUuid = str(uuid.uuid4()) theImage . labels_general . append ( label ) # 7. Send image to the server uuidImg = stub . TransferImage ( theImage ) print ( \"uuid of transfered img: \" + uuidImg . message ) # 8. Add coordinate transformations and send them to the server theTf = tf . TransformStamped () theTf . header . frame_id = \"map\" theTf . header . stamp . seconds = theTime theTf . header . uuid_project = projectuuid theTf . child_frame_id = \"camera\" theTf . transform . translation . x = 1 theTf . transform . translation . y = 2 theTf . transform . translation . z = 3 theTf . transform . rotation . x = 0 theTf . transform . rotation . y = 0 theTf . transform . rotation . z = 0 theTf . transform . rotation . w = 1 stubTf . TransferTransformStamped ( theTf ) theTf . header . stamp . seconds = theTime + 10 theTf . transform . translation . x = 100 theTf . transform . translation . y = 200 theTf . transform . translation . z = 300 stubTf . TransferTransformStamped ( theTf ) Output: uuid of transfered img: 5836f989-adbb-46a0-a689-9e0527d457fe uuid of transfered img: 5b39487d-238a-43e4-a8a4-0f7efb876e8b uuid of transfered img: 00ced216-40b1-4d54-817f-11c413b228c6 uuid of transfered img: 5d330208-e534-4bfb-b242-00295e6b027d uuid of transfered img: 18f00f33-0ac5-4221-a428-13e2c37b27cd uuid of transfered img: ed5134c8-da18-476f-9c1b-60cea04c5627 uuid of transfered img: 22c995b0-c2b6-4c94-85a3-b1118ae3ac7d uuid of transfered img: 9d980608-85ff-4e62-a6be-01c05bd59de9 uuid of transfered img: 24204e3f-1cbd-4cab-99b6-803b3944b450 uuid of transfered img: 797f2e1a-f9e8-4ed3-94b3-a5d101aa0697","title":"Sending images"},{"location":"tutorials/images/#query-images","text":"Now we want to query the previously send images with some criteria. Possible query parameters are: Bounding Boxes (spatial query) A time interval (temporal query) Labels (semantic query) Protocol Buffers Flatbuffers Source: examples/python/gRPC/images/gRPC_pb_queryImage.py #!/usr/bin/env python3 import os import sys import image_service_pb2_grpc as imageService import meta_operations_pb2_grpc as metaOperations import query_pb2 as query from google.protobuf import empty_pb2 # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service objects stub = imageService . ImageServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) # 2. Get all projects from the server response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # 3. Check if we have an existing test project, if not, we stop here projectuuid = \"\" for project in response . projects : print ( project . name + \" \" + project . uuid + \" \\n \" ) if project . name == \"testproject\" : projectuuid = project . uuid if projectuuid == \"\" : sys . exit () # 4. Create a query with parameters theQuery = query . Query () theQuery . projectuuid . append ( projectuuid ) theQuery . boundingboxstamped . header . frame_id = \"map\" theQuery . boundingboxstamped . boundingbox . point_min . x = 0.0 theQuery . boundingboxstamped . boundingbox . point_min . y = 0.0 theQuery . boundingboxstamped . boundingbox . point_min . z = 0.0 theQuery . boundingboxstamped . boundingbox . point_max . x = 100.0 theQuery . boundingboxstamped . boundingbox . point_max . y = 100.0 theQuery . boundingboxstamped . boundingbox . point_max . z = 100.0 # since epoche theQuery . timeinterval . time_min . seconds = 1638549273 theQuery . timeinterval . time_min . nanos = 0 theQuery . timeinterval . time_max . seconds = 1938549273 theQuery . timeinterval . time_max . nanos = 0 # labels theQuery . label . extend ([ \"testlabel0\" ]) # 5. Query the server for images matching the query and iterate over them for img in stub . GetImage ( theQuery ): print ( f \"uuidmsg: { img . header . uuid_msgs } \" ) print ( f \"first label: { img . labels_bb [ 0 ] . labelWithInstance . label } \" ) print ( \"First bounding box (Xmin, Ymin, Xmax, Ymax): \" + str ( img . labels_bb [ 0 ] . boundingBox . point_min . x ) + \" \" + str ( img . labels_bb [ 0 ] . boundingBox . point_min . y ) + \" \" + str ( img . labels_bb [ 0 ] . boundingBox . point_max . x ) + \" \" + str ( img . labels_bb [ 0 ] . boundingBox . point_max . y ) + \" \\n \" ) Output: testproject 3af70ba8-1e81-4f60-86d2-a4257d88f01e first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 first label: testlabel0 First bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04 Source: examples/images/gRPC/images/gRPC_fb_queryImage.py #!/usr/bin/env python3 import os import sys import flatbuffers from fb import Image from fb import image_service_grpc_fb as imageService # importing util functions. Assuming that these files are in the parent dir # examples/python/gRPC/util.py # examples/python/gRPC/util_fb.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util import util_fb builder = flatbuffers . Builder ( 1024 ) # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get all projects from the server projectuuid = util_fb . getProject ( builder , channel , 'testproject' ) # 2. Check if the defined project exist; if not exit if not projectuuid : exit () # 3. Get gRPC service object stub = imageService . ImageServiceStub ( channel ) # Create all necessary objects for the query header = util_fb . createHeader ( builder , frame = \"map\" ) pointMin = util_fb . createPoint ( builder , 0.0 , 0.0 , 0.0 ) pointMax = util_fb . createPoint ( builder , 100.0 , 100.0 , 100.0 ) boundingboxStamped = util_fb . createBoundingBoxStamped ( builder , header , pointMin , pointMax ) timeMin = util_fb . createTimeStamp ( builder , 1610549273 , 0 ) timeMax = util_fb . createTimeStamp ( builder , 1938549273 , 0 ) timeInterval = util_fb . createTimeInterval ( builder , timeMin , timeMax ) projectUuids = [ builder . CreateString ( projectuuid )] labels = [ builder . CreateString ( \"testlabel0\" )] dataUuids = [ builder . CreateString ( \"3e12e18d-2d53-40bc-a8af-c5cca3c3b248\" )] instanceUuids = [ builder . CreateString ( \"3e12e18d-2d53-40bc-a8af-c5cca3c3b248\" )] # 4. Create a query with parameters # all parameters are optional # with all parameters set (especially with the data and instance uuids set) the result # of the query will be empty. Set the query parameters to adequate values or remove # them from the query creation query = util_fb . createQuery ( builder , # boundingBox=boundingboxStamped, # timeInterval=timeInterval, # labels=labels, # projectUuids=projectUuids, # instanceUuids=instanceUuids, # dataUuids=dataUuids, withoutData = True , ) builder . Finish ( query ) buf = builder . Output () # 5. Query the server for images matching the query and iterate over them for responseBuf in stub . GetImage ( bytes ( buf )): response = Image . Image . GetRootAs ( responseBuf ) print ( f \"uuidmsg: { response . Header () . UuidMsgs () . decode ( 'utf-8' ) } \" ) print ( \"first label: \" + response . LabelsBb ( 0 ) . LabelWithInstance () . Label () . decode ( \"utf-8\" )) print ( \"first bounding box (Xmin,Ymin,Xmax,Ymax): \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMin () . X ()) + \" \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMin () . Y ()) + \" \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMax () . X ()) + \" \" + str ( response . LabelsBb ( 0 ) . BoundingBox () . PointMax () . Y ()) + \" \\n \" ) print ( \"done.\" ) Output: testproject 3af70ba8-1e81-4f60-86d2-a4257d88f01e uuidmsg: 00ced216-40b1-4d54-817f-11c413b228c6 first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 uuidmsg: 5836f989-adbb-46a0-a689-9e0527d457fe first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 uuidmsg: 5b39487d-238a-43e4-a8a4-0f7efb876e8b first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 uuidmsg: 5d330208-e534-4bfb-b242-00295e6b027d first label: testlabel0 first bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04 done.","title":"Query images"},{"location":"tutorials/overview/","text":"Tutorials Overviews The tutorials provide a starting point on how you can use SEEREP. Currently, the following topics are covered: Creating and Retrieving projects Sending and Querying images Before running any of the tutorials, make sure that you have a running SEEREP instance available . Local Instance To start SEEREP locally use STRG+SHIFT+D to open the Run & Debug Menu in Vs-Code, select seerep server and press run. Now a terminal should open and print the following info messages: Starting seerep server [ 2022 -08-01 13 :50:35.765427 ] <info>: The used logging folder is: /seerep/seerep-data/log/ [ 2022 -08-01 13 :50:35.765575 ] <info>: The used data folder is: /seerep/seerep-data/ [ 2022 -08-01 13 :50:35.765801 ] <info>: add the protobuf gRPC services... [ 2022 -08-01 13 :50:35.765860 ] <info>: add the flatbuffer gRPC services... [ 2022 -08-01 13 :50:35.767787 ] <info>: serving gRPC Server on \"[::]:9090\" ...","title":"Overview"},{"location":"tutorials/overview/#tutorials-overviews","text":"The tutorials provide a starting point on how you can use SEEREP. Currently, the following topics are covered: Creating and Retrieving projects Sending and Querying images Before running any of the tutorials, make sure that you have a running SEEREP instance available .","title":"Tutorials Overviews"},{"location":"tutorials/overview/#local-instance","text":"To start SEEREP locally use STRG+SHIFT+D to open the Run & Debug Menu in Vs-Code, select seerep server and press run. Now a terminal should open and print the following info messages: Starting seerep server [ 2022 -08-01 13 :50:35.765427 ] <info>: The used logging folder is: /seerep/seerep-data/log/ [ 2022 -08-01 13 :50:35.765575 ] <info>: The used data folder is: /seerep/seerep-data/ [ 2022 -08-01 13 :50:35.765801 ] <info>: add the protobuf gRPC services... [ 2022 -08-01 13 :50:35.765860 ] <info>: add the flatbuffer gRPC services... [ 2022 -08-01 13 :50:35.767787 ] <info>: serving gRPC Server on \"[::]:9090\" ...","title":"Local Instance"},{"location":"tutorials/projects/","text":"Creating & Retrieving Projects Creating new projects New projects for new data can be created in the following way: Source: examples/gRPC/meta/gRPC_pb_createProject.py import os import sys import meta_operations_pb2_grpc as metaOperations import projectCreation_pb2 # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service object stub = metaOperations . MetaOperationsStub ( channel ) # 2. Create the project object inline and send it to the server response = stub . CreateProject ( projectCreation_pb2 . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" )) print ( \"The new project on the server is (name/uuid):\" ) print ( \" \\t \" + response . name + \" \" + response . uuid ) Output: The new project on the server is ( name/uuid ) : testproject eff47bc9-c39e-430e-8153-88e0eab65768 Retrieving projects After we created two projects, we can query them. Currently the name doesn't have to be unique. Protocol Buffers Flatbuffers Source: examples/gRPC/meta/gRPC_pb_getProjects.py import os import sys import meta_operations_pb2_grpc as metaOperations from google.protobuf import empty_pb2 # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service object stub = metaOperations . MetaOperationsStub ( channel ) # 2. Get all projects on the server response = stub . GetProjects ( empty_pb2 . Empty ()) print ( \"The server has the following projects (name/uuid):\" ) for projectinfo in response . projects : print ( \" \\t \" + projectinfo . name + \" \" + projectinfo . uuid ) Output: The Server has the following projects (name/uuid): testproject 5c1ed18e-9180-40e1-a79b-594f8266d898 testproject 9fc3011f-4a3c-400e-9170-06973a6fb395 Source: examples/gRPC/meta/gRPC_fb_getProjects.py import os import sys import flatbuffers from fb import Empty , ProjectInfos from fb import meta_operations_grpc_fb as metaOperations # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service object stub = metaOperations . MetaOperationsStub ( channel ) # Create an empty message builder = flatbuffers . Builder ( 1024 ) Empty . Start ( builder ) emptyMsg = Empty . End ( builder ) builder . Finish ( emptyMsg ) buf = builder . Output () # 2. Get all projects on the server responseBuf = stub . GetProjects ( bytes ( buf )) response = ProjectInfos . ProjectInfos . GetRootAs ( responseBuf ) print ( \"The server has the following projects (name/uuid):\" ) for i in range ( response . ProjectsLength ()): print ( \" \\t \" + response . Projects ( i ) . Name () . decode ( \"utf-8\" ) + \" \" + response . Projects ( i ) . Uuid () . decode ( \"utf-8\" )) Output: The server has the following projects (name/uuid): testproject 5c1ed18e-9180-40e1-a79b-594f8266d898 testproject 9fc3011f-4a3c-400e-9170-06973a6fb395","title":"Creating & retrieving projects"},{"location":"tutorials/projects/#creating-retrieving-projects","text":"","title":"Creating &amp; Retrieving Projects"},{"location":"tutorials/projects/#creating-new-projects","text":"New projects for new data can be created in the following way: Source: examples/gRPC/meta/gRPC_pb_createProject.py import os import sys import meta_operations_pb2_grpc as metaOperations import projectCreation_pb2 # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service object stub = metaOperations . MetaOperationsStub ( channel ) # 2. Create the project object inline and send it to the server response = stub . CreateProject ( projectCreation_pb2 . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" )) print ( \"The new project on the server is (name/uuid):\" ) print ( \" \\t \" + response . name + \" \" + response . uuid ) Output: The new project on the server is ( name/uuid ) : testproject eff47bc9-c39e-430e-8153-88e0eab65768","title":"Creating new projects"},{"location":"tutorials/projects/#retrieving-projects","text":"After we created two projects, we can query them. Currently the name doesn't have to be unique. Protocol Buffers Flatbuffers Source: examples/gRPC/meta/gRPC_pb_getProjects.py import os import sys import meta_operations_pb2_grpc as metaOperations from google.protobuf import empty_pb2 # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service object stub = metaOperations . MetaOperationsStub ( channel ) # 2. Get all projects on the server response = stub . GetProjects ( empty_pb2 . Empty ()) print ( \"The server has the following projects (name/uuid):\" ) for projectinfo in response . projects : print ( \" \\t \" + projectinfo . name + \" \" + projectinfo . uuid ) Output: The Server has the following projects (name/uuid): testproject 5c1ed18e-9180-40e1-a79b-594f8266d898 testproject 9fc3011f-4a3c-400e-9170-06973a6fb395 Source: examples/gRPC/meta/gRPC_fb_getProjects.py import os import sys import flatbuffers from fb import Empty , ProjectInfos from fb import meta_operations_grpc_fb as metaOperations # importing util functions. Assuming that this file is in the parent dir # examples/python/gRPC/util.py script_dir = os . path . dirname ( __file__ ) util_dir = os . path . join ( script_dir , '..' ) sys . path . append ( util_dir ) import util # Default server is localhost ! channel = util . get_gRPC_channel () # 1. Get gRPC service object stub = metaOperations . MetaOperationsStub ( channel ) # Create an empty message builder = flatbuffers . Builder ( 1024 ) Empty . Start ( builder ) emptyMsg = Empty . End ( builder ) builder . Finish ( emptyMsg ) buf = builder . Output () # 2. Get all projects on the server responseBuf = stub . GetProjects ( bytes ( buf )) response = ProjectInfos . ProjectInfos . GetRootAs ( responseBuf ) print ( \"The server has the following projects (name/uuid):\" ) for i in range ( response . ProjectsLength ()): print ( \" \\t \" + response . Projects ( i ) . Name () . decode ( \"utf-8\" ) + \" \" + response . Projects ( i ) . Uuid () . decode ( \"utf-8\" )) Output: The server has the following projects (name/uuid): testproject 5c1ed18e-9180-40e1-a79b-594f8266d898 testproject 9fc3011f-4a3c-400e-9170-06973a6fb395","title":"Retrieving projects"}]}