{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"data-protection-notice/","title":"Data Protection Notice","text":"<p>The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data. In this data protection policy we inform you about the processing of your personal data when visiting and using our web site.</p>"},{"location":"data-protection-notice/#controller","title":"Controller","text":"<p>Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Phone: +49 631 20575 0 info@dfki.de Legal Notice</p>"},{"location":"data-protection-notice/#data-protection-officer","title":"Data protection officer","text":"<p>Phone: +49 631 20575 0 datenschutz@dfki.de</p>"},{"location":"data-protection-notice/#hosting-server","title":"Hosting Server","text":"<p>This website is hosted with Github. For more information about github's data processing and contacts, please refer to the Privacy Policy.</p>"},{"location":"data-protection-notice/#access-and-intervention","title":"Access and Intervention","text":"<p>Besides the information in this data protection policy you have the right of access to your personal data. To ensure fair data processing, you have the following rights:</p> <ul> <li>The right to rectification and completion of your personal data</li> <li>The right to erasure of your personal data</li> <li>The right to restriction of the processing of your personal data</li> <li>The right to object to the processing of your personal data on grounds related to your particular situation</li> </ul> <p>To exercise these rights, please contact our data protection officer.</p>"},{"location":"data-protection-notice/#right-to-lodge-a-complaint","title":"Right to lodge a complaint","text":"<p>You have the right to lodge a complaint with a supervisory authority if you consider that the processing of your personal data infringes statutory data protection regulations.</p>"},{"location":"legal-notice/","title":"LEGAL NOTICE","text":""},{"location":"legal-notice/#responsible-service-provider","title":"Responsible service provider","text":"<p>Responsible for the content of the domain agri-gaia.github.io/seerep from the point of view of \u00a7 5 TMG:</p> <p>Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Kr\u00fcger Helmut Ditzer Trippstadter Str. 122 67663 Kaiserslautern Germany</p> <p>Phone: +49 631 20575 0 Email: info@dfki.de</p> <p>Register Court: Amtsgericht Kaiserslautern Register Number: HRB 2313</p> <p>ID-Number: DE 148 646 973</p> <p>The person responsible for the editorial content of the domain agri-gaia.github.io/seerep of the German Research Center for Artificial Intelligence GmbH within the meaning of \u00a7 18 para. 2 MStV is:</p> <p>Mark Niemeyer Berghoffstra\u00dfe 11 49090 Osnabr\u00fcck Germany</p> <p>Phone: +49 541 386050 2254 E-mail: mark.niemeyer@dfki.de Website URL: www.dfki.de</p>"},{"location":"legal-notice/#liability-for-content","title":"Liability for content","text":"<p>As a service provider, Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) is responsible under general law for its own content published on this website in accordance with Section 7 para. 1 of the German Telemedia Act (TMG).</p> <p>DFKI makes every effort to keep the information on our website accurate and current, nevertheless, errors and uncertainties cannot be entirely ruled out. For this reason, DFKI undertakes no liability for ensuring that the provided information is current, accurate or complete, and is not responsible for its quality. DFKI is not liable for material or immaterial damages caused directly or indirectly by the use or non-use of the offered information, or by the use of erroneous and incomplete information, unless willful or grossly negligent fault can be demonstrated. This also applies with respect to software or data provided for download.</p> <p>DFKI reserves the right to modify, expand or delete parts of the website or the entire website without separate announcement, or to cease publication temporarily or definitively.</p>"},{"location":"legal-notice/#liability-for-links","title":"Liability for links","text":"<p>Pursuant to Section 7 para. 1 of TMG (German Tele-Media Act), the law limits our responsibility as a service provider to our own content on this website. According to Sections 8 \u2013 10 TMG, we are not obliged to permanently monitor any transmitted or stored third-party information, nor to investigate circumstances that might indicate illegal activity. This does not affect our obligation to remove or block information according to general law. However, we can only assume liability for such data from the point in time at which a concrete legal infringement has been identified. Upon notification of any such legal infringement, we will immediately delete the infringing content.</p> <p>Cross-references (\u201clinks\u201d) to the content providers are to be distinguished from our own content. Our offer includes links to external third-party websites. Providers or operators of linked external pages are always responsible for their respective content. We cannot assume any liability for the content of the linked pages. This third-party content was checked by DFKI when the links were first set up to determine whether any legal infringements existed. At the time of the check, no legal infringements were apparent. However, it cannot be ruled out that the content is subsequently changed by the respective providers. A permanent control of the content of the linked pages is not reasonable without evidence of a legal infringement. Should you believe that the linked external pages infringe applicable law or otherwise contain inappropriate content, please notify us directly at: info@dfki.de.</p> <p>In case DFKI should notice or receive any indication that an external offer to which it has linked might cause civil or criminal liability, DFKI will immediately delete this link.</p>"},{"location":"legal-notice/#copyright","title":"Copyright","text":"<p>The layout of the homepage, the graphics used and other content on the DFKI website are protected by copyright. The reproduction, processing, distribution and any type of use outside the boundaries of copyright law require the prior written approval of the DFKI. Insofar as any content on this page was not created by DFKI, the copyrights of third parties will be respected. If you believe that you discovered a copyright infringement, please report this to us accordingly. Upon becoming aware of any legal infringements, DFKI will remove or disable access to such infringing content immediately.</p>"},{"location":"getting-started/client/pypi/","title":"SEEREP gRPC API","text":"<p>For easy communication to the server via Python the needed Python files for the gRPC API are available on pypi in the seerep-grpc project. Install them with:</p> <pre><code>pip install seerep-grpc\n</code></pre>"},{"location":"getting-started/client/pypi/#usage","title":"Usage","text":"<p>Import the classes from the <code>seerep.pb</code> or <code>seerep.fb</code> modules like this:</p> <pre><code>from seerep.pb import image_pb2 as image\nfrom seerep.pb import image_service_pb2_grpc as imageService\n</code></pre> <pre><code>from seerep.fb import Image\nfrom seerep.fb import image_service_grpc_fb as imageService\n</code></pre> <p>For more advanved examples take a look at the Tutorial or at the SEEREP repository.</p>"},{"location":"getting-started/server/configuration/","title":"Configure the server","text":"<p>When starting the server some options to configure the server are available. The configuration is possible via the command line, via a config file or via environment variables.</p>"},{"location":"getting-started/server/configuration/#command-line","title":"Command line","text":"<p>For the command line the following options are available. All of them are optional and have default values. The latest list of options can always be retrieved by starting the server with the <code>--help</code> argument.</p> <pre><code>Generic options:\n  -v [ --version ]                      print version string\n  --help                                produce help message\n  -c [ --config ] arg                   name of a file of a configuration.\n\nConfiguration:\n  -D [ --data-folder ] arg (=/seerep/devel/bin)\ndata folder\n  -L [ --log-path ] arg                 log path\n  --log-level arg (=info)               log-level [trace, debug, info, warning,\n                                        error, fatal]\n-p [ --port ] arg (=9090)             gRPC port\n</code></pre>"},{"location":"getting-started/server/configuration/#config-file","title":"Config file","text":"<p>The configuration options of the command line options can also be set via a config file. The path and name of the config file has to be given to the server using the <code>--config</code> option.</p> <p>An example config file:</p> <pre><code>data-folder = /seerep/seerep-data/ #defaulting to work dir\nlog-path = /seerep/seerep-data/log/ #file logging disabled if not set\nlog-level = info\nport = 9090\n</code></pre>"},{"location":"getting-started/server/configuration/#environment-variables","title":"Environment Variables","text":"<p>If one of the following environment variables is set, it will be parsed and set as the command line equivalent.</p> Environment variable command line equivalent SEEREP_DATA_FOLDER --data-folder SEEREP_LOG_PATH --log-path SEEREP_LOG_LEVEL --log-level SEEREP_PORT --port"},{"location":"getting-started/server/docs/","title":"Docs","text":"<p>The documentation is published via GitHub Pages. The responsible workflow builds <code>Doxygen</code> and <code>MkDocs</code> and publishes them on the <code>gh-pages</code> branch. MkDocs focusses on higher level concepts like the installation process and a package overview, while Doxygen is used for code documentation. If you want to work on the documentation locally i.e for a PR follow the steps below.</p>"},{"location":"getting-started/server/docs/#dependencies","title":"Dependencies","text":"<p>If you are not using the SEEREP development container, you need to have <code>doxygen</code> and <code>MkDocs-Material</code> installed, use the commands below:</p> <pre><code>pip3 mkdocs-material\nsudo apt install doxygen\n</code></pre>"},{"location":"getting-started/server/docs/#mkdocs","title":"MkDocs","text":"<p>To run MkDocs locally switch into the main directory of SEEREP, where the <code>mkdocs.yml</code> is located. Then use <code>mkdocs serve</code> to build and deploy MkDocs on a local http-server. The page should then be available under http://127.0.0.1:8000/.</p>"},{"location":"getting-started/server/docs/#doxygen","title":"Doxygen","text":"<p>To create the Doxygen output locally switch into the main directory of SEEREP, where the <code>Doxyfile</code> is located and run <code>doxygen Doxyfile</code>. Now an <code>doxygen/html/</code> folder should be in the same directory, switch into it.</p> <p>If you are not working in the development container you can simply open the <code>index.html</code> with your browser of choice (e.g. <code>firefox index.html</code>). Otherwise, use <code>python3 -m http.server</code> to start a local web server which serves the content. The page should be available under the default http://0.0.0.0:8000/ address.</p> <p>If you want to run MkDocs and Doxygen at the same time you need to provide a different port to the Doxygen http-server, use <code>python3 -m http.server 8002</code> instead.</p>"},{"location":"getting-started/server/installationDev/","title":"Development Environment Installation","text":"<p>This page provides an overview on how to install the SEEREP development environment.</p>"},{"location":"getting-started/server/installationDev/#vs-code-development-container","title":"VS-Code Development Container","text":"<ul> <li>The VS-Code Development Container is the easiest and recommended way to develop   SEEREP.</li> </ul>"},{"location":"getting-started/server/installationDev/#requirements","title":"Requirements","text":"<ul> <li>Current Version of VS-Code</li> <li>Docker &gt;= 17.12.0</li> </ul>"},{"location":"getting-started/server/installationDev/#development-container-setup","title":"Development Container Setup","text":"<ol> <li> <p>Clone the SEEREP repository from    Github and open it in VS-Code.</p> <pre><code>git clone https://github.com/agri-gaia/seerep\ncd seerep/\ncode .\n</code></pre> </li> <li> <p>Create a sibling folder next to the repo called <code>seerep-data</code>. This folder    will be mounted for the data exchange between host and container. Without    this folder, the following steps will fail!</p> <pre><code>mkdir ../seerep-data\n</code></pre> </li> <li> <p>Install the Remote    Containers    and    Docker    VS-Code extension with the following commands or via the extensions tab in Vs-Code.</p> <pre><code>code --install-extension ms-vscode-remote.remote-containers\ncode --install-extension ms-azuretools.vscode-docker\n</code></pre> </li> <li> <p>Press <code>F1</code> or <code>CTRL + SHIFT + P</code> in VS-Code and enter <code>Remote-Containers:    Reopen Folder in Container</code>. The installation process can take a couple of    minutes since, the docker image of SEEREP is downloaded and started.    Additionally, all necessary VS Code extensions are installed inside the    container and Intellisense, pre-commit hooks are    set up.VS-Code may ask you to login to GitHub, to get the latest    updates from the repository.</p> </li> </ol>"},{"location":"getting-started/server/installationDev/#credentials","title":"Credentials","text":"<p>The default username and password for the Docker container are:</p> <ul> <li>user:<code>docker</code></li> <li>password: <code>docker</code>.</li> </ul>"},{"location":"getting-started/server/installationDev/#pre-commit-checks","title":"Pre Commit Checks","text":"<p>This repository uses pre-commit checks to identify simple issues in the code base. The checks are automatically run before each commit. If you want to run the pre-commit checks during the development of a commit, use <code>pre-commit run -a</code>.</p>"},{"location":"getting-started/server/installationDev/#hints-to-fix-errors","title":"Hints To Fix Errors","text":"<p>If the setup or the <code>Remote-Containers: Reopen Folder in Container</code> fails, here are a couple of hints on how to fix them.</p> <ol> <li> <p>First make sure that the Docker container is not already running, use <code>docker    container stop $VSC_SEEREP_CONTAINER_ID</code>, the container ID can be found using    <code>docker ps</code>.</p> </li> <li> <p>Additionally, you can delete all the data regarding SEEREP, to get a fresh    installation:</p> <pre><code>docker volume rm seerep-vscode-extensions\ndocker volume rm vscode\ndocker rmi ghcr.io/agri-gaia/seerep_base:latest\n(docker rmi ghcr.io/agri-gaia/seerep_server:latest)\ndocker rmi vsc-seerep-*\n</code></pre> </li> </ol>"},{"location":"getting-started/server/installationDev/#manual-installation","title":"Manual Installation","text":"<p>It is not recommended to install the following dependencies globally. Some of them are really hard to uninstall. If you still want to install SEEREP in this way, follow the steps:</p> <ol> <li>Install ROS Noetic with the official    documentation</li> <li>Install SEEREPs dependencies:    gRPC, Protocol    Buffers,    Flatbuffers,    HighFive.    Therefore, please follow the steps in the base    Dockerfile.</li> </ol> <p>In order to build SEEREP, we recommend the common build tool from ROS, catkin. Follow the next steps to download and build seerep globally on your system.</p> <pre><code>source /opt/ros/noetic/setup.bash\nmkdir -p seerep_ws/src\ncd seerep_ws/src\ngit clone https://github.com/agri-gaia/seerep.git\ncd ..\ncatkin build\n</code></pre>"},{"location":"getting-started/server/kubernetes-deployment/","title":"Kubernetes Deployment","text":"<p>Besides, the local installation and the usage of an available docker container (see installation), one can also deploy the seerep-server within a kubernetes cluster.</p>"},{"location":"getting-started/server/kubernetes-deployment/#relevant-files","title":"Relevant files","text":"<p>Seerep can either be installed with the latest development state or the latest stable version. The relevant files can be found under</p> <ul> <li>/docker/kustomize/base --&gt; development</li> <li>/docker/kustomize/overlays/production --&gt; latest stable release</li> </ul> <p>The base-folder contains all yaml-files for a cluster deployments. This includes</p> <ul> <li>Deployment</li> <li>PersistentVolume and PersistentVolumeClaim</li> <li>Service</li> <li>Ingress</li> <li>Configuration</li> </ul> <p>The yaml-file for the cluster is create via Kustomize, hence this folder also contains a kustomization.yaml which puts everything together.</p> <p>The /overlay/production folder contains a second kustomization.yaml. Within this file everything needed to install a production system is overridden. This means, that the sealed secret is replaced with a new one, while the base secret is deleted. Further, the labels and the names of PV and PVC are changed to create new storage explicitly for the production system. Finally, the used image is replaced with the latest stable release.</p> <p>The usage of an overlay, thereby, follows the principles of Kustomize.</p>"},{"location":"getting-started/server/kubernetes-deployment/#building-with-kustomize","title":"Building with Kustomize","text":"<p>Before one can build the kubernetes manifest, one needs to install Kustomize (Kustomize installation) The easiest way to that is the download the latest binary from the offical release page</p> <p>To install the base-version of seerep either one can run:</p> <pre><code>kustomize build base/ &gt; seerep-deployment.yaml\n</code></pre> <p>to store the manifest in a separate yaml file. Or directly use kubectl:</p> <pre><code>kubectl apply -k base/\n</code></pre> <p>In order to install the production version, the commands look slightly different:</p> <pre><code>kustomize build overlays/production/ &gt; seerep-deployment.yaml\n</code></pre> <pre><code>kubectl apply -k overlays/production/\n</code></pre> <p>If someone has a running ArgoCD instance, it is also possible to integrate seerep as a project into ArgoCD.</p>"},{"location":"getting-started/server/kubernetes-deployment/#sealed-secrets","title":"Sealed Secrets","text":"<p>The certificates used for the secured ingress are created as sealed-secret. Hence, the secret can safely be stored in a repository. The sealed secret controller installed within the cluster will take care of unsealing the secret and make it usable. To combine Kustomize and sealed secrets this blogs-post was followed faun.pub</p>"},{"location":"getting-started/server/local-deployment/","title":"Local Deployment","text":"<p>The local deployment is based on the seerep_server docker image. The image with the latest (unstable) version can be pulled with the following command. It is recommended to use a version tag instead of <code>latest</code>.</p> <pre><code>docker pull ghcr.io/agri-gaia/seerep_server:latest\n</code></pre>"},{"location":"getting-started/server/local-deployment/#docker-run","title":"docker run","text":"<p>Run the following command to start the server using <code>docker run</code>. It is recommended to use a version tag instead of <code>latest</code>.</p> <pre><code>docker run \\\n--volume=seerep-data:/mnt/seerep-data \\\n--publish=9090:9090 \\\n--name=seerep_server \\\n--tty \\\nghcr.io/agri-gaia/seerep_server:latest \\\n--data-folder=/mnt/seerep-data\n</code></pre>"},{"location":"getting-started/server/local-deployment/#docker-compose","title":"docker-compose","text":"<p>Run <code>docker-compose up</code> in the folder of the docker-compose.yml to start the server. It is recommended to use a version tag instead of <code>latest</code>.</p> <p>For this docker compose has to be installed. In the latest version <code>docker compose</code> without a hyphen as part of the Docker CLI replaces <code>docker-compose</code>.</p> <p>Example docker-compose.yml:</p> <pre><code>version: \"3.6\"\nservices:\n  seerep:\n    image: ghcr.io/agri-gaia/seerep_server:latest\n    tty: true\n    container_name: seerep_server\n    ports:\n      # the gRPC port\n      - 9090:9090\n    volumes:\n      # persist the data folder\n      - seerep-data:/mnt/seerep-data #using docker volume\n      #- /your/local/absolute/path:/mnt/seerep-data #using host folder\n    environment:\n      - TZ=Europe/Berlin\n      - SEEREP_DATA_FOLDER=/mnt/seerep-data\n      - SEEREP_LOG_PATH=/mnt/seerep-data/log\n      - SEEREP_LOG_LEVEL=info\nvolumes:\n  seerep-data:\n</code></pre>"},{"location":"getting-started/server/tests/","title":"Tests","text":"<p>We are currently working on integrating more tests into SEEREP. GoogleTest is used as  a testing framework. The tests are run in a GitHub workflow for every PR and push to the main branch, the tests can also be run locally in a couple of different ways.</p>"},{"location":"getting-started/server/tests/#running-tests-locally","title":"Running tests locally","text":""},{"location":"getting-started/server/tests/#catkin","title":"Catkin","text":"<p>The tests can be run via <code>catkin</code> in the command line. When run without a specific package, all tests in the workspace are executed. But while this is very convenient, catkin does not provide much information/output if a test fails.</p> <pre><code>catkin test (&lt;specific-package&gt;)\n</code></pre>"},{"location":"getting-started/server/tests/#vs-code","title":"Vs-Code","text":"<p>Another way to run the test is via the Vs-Code test explorer (triangle test-tube on the left bar of VS-Code). If you have done a fresh installation of the project, it can happen, that the test cases won't be recognized. In order to fix that, just restart the development container. For that, you can use <code>Reopen Folder Locally</code> and then <code>Reopen In Container</code> again. Now you should be able to see the test cases as, in the example below:</p> <p></p> <p>The icons in the top of the test explorer are mostly self-explanatory, refresh, all tests can be run, a single test can be debugged, and a terminal can be opened to print the output of the tests.</p>"},{"location":"getting-started/server/tests/#executables","title":"Executables","text":"<p>If you would like to run the tests via their executables, they are located under <code>/seerep/devel/bin/&lt;test-name&gt;</code> or <code>/seerep/build/&lt;package&gt;/&lt;test-name&gt;</code>.</p>"},{"location":"home/","title":"Home","text":"<p>The objective of SEEREP (SEmantic Environment REPresentation) is to store generated robot data and enable fast spatio-temporal-semantic queries over the data.</p>"},{"location":"home/#context","title":"Context","text":"<p>Autonomous robotic systems must be aware of their environment, in order to safely achieve their goal-oriented actions. Especially in unstructured and changing environments, a detailed model of the environment is required for reasoning and planning.</p> <p>The sensors of a robot provide spatial information via the robot's pose, temporal information is created by the point in time when a sensor is read. Semantic information always exists implicitly and can be made explicit by algorithms or manual labeling.</p> <p>Most existing environment representations focus on one or two of these information types, SEEREP is able to store all three. Thereby, SEEREP enables the robot to reason on a higher level and disambiguate sensor data based on the context.</p>"},{"location":"home/#core-features","title":"Core Features","text":"<ul> <li>Store robotic sensor data</li> <li>Images</li> <li>3D-Points</li> <li>Point Clouds</li> <li>TF</li> <li>Store analysis results alongside the corresponding sensor data</li> <li>Add (bounding box based) semantic annotations with confidences to the data</li> <li>Extend semantic annotations by object instances which are in multiple datasets</li> <li>Store information about an instance alongside the data</li> <li>Link 3d points to an instance to define the instance position</li> <li>Allow multiple categories of labels (e.g. a category per CNN). Thus, enabling easy evaluation and comparison of   multiple CNNs based on the same data</li> <li>Fast spatio-temporal-semantic queries with   gRPC.</li> <li>Storage of data generated by the robotic system:</li> <li>Offline on the robot (no or slow internet connection), currently in Progress         #89</li> <li>Online on a server-cluster, with         gRPC.</li> <li>Shifting computation loads away from the robot and into the cloud.</li> <li>Easily switch between Protocol Buffers   (PB) /   Flatbuffers (FB) as the messaging   format.</li> </ul>"},{"location":"home/#architecture","title":"Architecture","text":"<p>The following graphic provides a broad overview of SEEREPs components. The sensor data along with results from processing the data and annotations are stored in HDF5 files. SEEREP uses projects to group common information (e.g. a scanning campaign). The data can be saved locally on the robot or sent into the cloud with gRPC. Therefore, the computational load on the robot can be reduced and algorithms can fetch the data subset which they actually need.</p> <p></p> <p>A more detailed version, with all ROS packages and message types is available in the package overview.</p>"},{"location":"reference/packages/","title":"Package Overview","text":"<p>This page provides an overview of all the ROS-Packages used in SEEREP.</p>"},{"location":"reference/packages/#general-seerep-structure","title":"General SEEREP Structure","text":"<p>The general structure of SEEREP is schematically illustrated in the following graphic.</p> <p>SEEREP can be split into two parts, one of which runs on the robot (left box) and one which runs on a server clusters (right box). The communication is handled via gRPC and protocol-buffers (PB) or flatbuffers (FB).</p> <p></p>"},{"location":"reference/packages/#packages","title":"Packages","text":"<p>In the following, each package will be described in more detail.</p>"},{"location":"reference/packages/#seerep-hdf5","title":"seerep-hdf5","text":"<p>The\u00a0<code>seerep-hdf5</code>\u00a0unit provides access to the hdf5 files to store or retrieve data. The unit is split into three packages\u00a0<code>seerep-hdf5-core</code>,\u00a0<code>seerep-hdf5-pb</code>\u00a0and\u00a0<code>seerep-hdf5-fb</code>. This is to have a server-core which is independent of the message format, so that it's possible to easily switch between PB and FB or any other message format.</p> <ul> <li> <p>The main task for the <code>seerep-core</code> is to read UUIDs, bounding boxes (BB),   time/semantic information on provided indices from the hdf5-files. The only   write operation of the core is to create new hdf5-files. Due to the   independence of FB and PB, new communication-messages are added to seerep-msgs   (<code>seerep-msgs/core</code>).</p> </li> <li> <p><code>seerep-hdf5-pb</code> and <code>seerep-hdf5-fb</code> provide methods to read or write   point clouds, images and transformations from PB or FB  messages.</p> </li> </ul>"},{"location":"reference/packages/#seerep-srv","title":"seerep-srv","text":"<p>The <code>seerep-srv</code> is split into four parts <code>seerep-server</code>, <code>seerep-core</code> and <code>seerep-core-pb</code>, <code>seerep-core-fb</code>.</p> <ul> <li> <p>The <code>seerep-server</code> provides the top level interface for the SEEREP server   cluster, services which clients can be registered here. The server passes   request to the corresponding unit in the layer below (see graphic).</p> </li> <li> <p>The <code>seerep-core-pb</code> / <code>seerep-core-fb</code>  writes incoming PB / FB messages to   the hdf5 files. In case of a query the <code>seerep-core</code> is asked for the UUIDs of   the datasets which match the query parameters.</p> </li> </ul>"},{"location":"reference/packages/#seerep-msgs","title":"seerep-msgs","text":"<p>The <code>seerep-msgs</code> package defines all the PB, FB and core messages used in SEEREP.</p>"},{"location":"reference/packages/#seerep-ros","title":"seerep-ros","text":"<p><code>seerep-ros</code> provides three packages which run on the robot itself. The <code>seerep_ros_conversions_pb/fb</code> packages simply convert ROS messages to PB/FB and vice versa. The second package <code>seerep_ros_communication</code> is used to save sensor information like images and point clouds on the robot, or in case of a good internet connection to the remote server-cluster. Further, the robot is able to query the server for information to support his understanding of the environment and aid its navigation.</p> <ul> <li> <p>The <code>seerep_ros_communication\\client</code> is responsible for sending sensor   information directly to the remote server.</p> </li> <li> <p>The <code>seerep_ros_communication\\querier</code> is used to get information from the   remote server.</p> </li> <li> <p><code>seerep_ros_communication\\hdf5-dump</code> is used to save sensor information on a   hard drive which is located on the robot.</p> </li> </ul>"},{"location":"reference/packages/#seerep-com","title":"seerep-com","text":"<p><code>seerep-com</code> is used to define the gRPC services in PB and FB.</p>"},{"location":"tutorials/images/","title":"Sending &amp; Querying images","text":""},{"location":"tutorials/images/#sending-images","title":"Sending images","text":"<p>In this example we want to send images with labeled bounding boxes as well as general labels to SEEREP. Additionally we add some coordinate transformations at the end.</p> <p>Source: <code>examples/python/gRPC/images/gRPC_pb_sendLabeledImage.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nimport sys\nimport time\nimport uuid\nimport boundingbox2d_labeled_pb2 as bb\nimport image_pb2 as image\nimport image_service_pb2_grpc as imageService\nimport label_with_instance_pb2 as labelWithInstance\nimport meta_operations_pb2_grpc as metaOperations\nimport numpy as np\nimport projectCreation_pb2 as projectCreation\nimport tf_service_pb2_grpc as tfService\nimport transform_stamped_pb2 as tf\nfrom google.protobuf import empty_pb2\nscript_dir = os.path.dirname(__file__)\nutil_dir = os.path.join(script_dir, '..')\nsys.path.append(util_dir)\nimport util\n# Default server is localhost !\nchannel = util.get_gRPC_channel(target = \"local\")\n# 1. Get gRPC service objects\nstub = imageService.ImageServiceStub(channel)\nstubTf = tfService.TfServiceStub(channel)\nstubMeta = metaOperations.MetaOperationsStub(channel)\n# 2. Get all projects from the server\nresponse = stubMeta.GetProjects(empty_pb2.Empty())\n# 3. Check if we have an existing test project, if not, one is created.\nfound = False\nfor project in response.projects:\nprint(project.name + \" \" + project.uuid)\nif project.name == \"testproject\":\nprojectuuid = project.uuid\nfound = True\nif not found:\ncreation = projectCreation.ProjectCreation(name=\"testproject\", mapFrameId=\"map\")\nprojectCreated = stubMeta.CreateProject(creation)\nprojectuuid = projectCreated.uuid\ntheTime = int(time.time())\n# 4. Create ten images\nfor n in range(10):\ntheImage = image.Image()\nrgb = []\nlim = 256 # 256 x 256 pixels\nfor i in range(lim):\nfor j in range(lim):\nx = float(i) / lim\ny = float(j) / lim\nz = float(j) / lim\nr = np.ubyte((x * 255.0 + n) % 255)\ng = np.ubyte((y * 255.0 + n) % 255)\nb = np.ubyte((z * 255.0 + n) % 255)\nrgb.append(r)\nrgb.append(g)\nrgb.append(b)\n# Add image meta-data\ntheImage.header.frame_id = \"camera\"\ntheImage.header.stamp.seconds = theTime + n\ntheImage.header.stamp.nanos = 0\ntheImage.header.uuid_project = projectuuid\ntheImage.height = lim\ntheImage.width = lim\ntheImage.encoding = \"rgb8\"\ntheImage.step = 3 * lim\n# Add image data\ntheImage.data = bytes(rgb)\n# 5. Create bounding boxes with labels\nbb1 = bb.BoundingBox2DLabeled()\nfor i in range(0, 2):\nbb1.labelWithInstance.label = \"testlabel\" + str(i)\nbb1.labelWithInstance.instanceUuid = str(uuid.uuid4())\nbb1.boundingBox.point_min.x = 0.01 + i / 10\nbb1.boundingBox.point_min.y = 0.02 + i / 10\nbb1.boundingBox.point_max.x = 0.03 + i / 10\nbb1.boundingBox.point_max.y = 0.04 + i / 10\ntheImage.labels_bb.append(bb1)\n# 6. Add general labels to the image\nfor i in range(0, 2):\nlabel = labelWithInstance.LabelWithInstance()\nlabel.label = \"testlabelgeneral\" + str(i)\n# assuming that that the general labels are not instance related -&gt; no instance uuid\n# label.instanceUuid = str(uuid.uuid4())\ntheImage.labels_general.append(label)\n# 7. Send image to the server\nuuidImg = stub.TransferImage(theImage)\nprint(\"uuid of transfered img: \" + uuidImg.message)\n# 8. Add coordinate transformations and send them to the server\ntheTf = tf.TransformStamped()\ntheTf.header.frame_id = \"map\"\ntheTf.header.stamp.seconds = theTime\ntheTf.header.uuid_project = projectuuid\ntheTf.child_frame_id = \"camera\"\ntheTf.transform.translation.x = 1\ntheTf.transform.translation.y = 2\ntheTf.transform.translation.z = 3\ntheTf.transform.rotation.x = 0\ntheTf.transform.rotation.y = 0\ntheTf.transform.rotation.z = 0\ntheTf.transform.rotation.w = 1\nstubTf.TransferTransformStamped(theTf)\ntheTf.header.stamp.seconds = theTime + 10\ntheTf.transform.translation.x = 100\ntheTf.transform.translation.y = 200\ntheTf.transform.translation.z = 300\nstubTf.TransferTransformStamped(theTf)\n</code></pre> <p>Output:</p> <pre><code>uuid of transfered img: 5836f989-adbb-46a0-a689-9e0527d457fe\nuuid of transfered img: 5b39487d-238a-43e4-a8a4-0f7efb876e8b\nuuid of transfered img: 00ced216-40b1-4d54-817f-11c413b228c6\nuuid of transfered img: 5d330208-e534-4bfb-b242-00295e6b027d\nuuid of transfered img: 18f00f33-0ac5-4221-a428-13e2c37b27cd\nuuid of transfered img: ed5134c8-da18-476f-9c1b-60cea04c5627\nuuid of transfered img: 22c995b0-c2b6-4c94-85a3-b1118ae3ac7d\nuuid of transfered img: 9d980608-85ff-4e62-a6be-01c05bd59de9\nuuid of transfered img: 24204e3f-1cbd-4cab-99b6-803b3944b450\nuuid of transfered img: 797f2e1a-f9e8-4ed3-94b3-a5d101aa0697\n</code></pre>"},{"location":"tutorials/images/#query-images","title":"Query images","text":"<p>Now we want to query the previously send images with some criteria. Possible query parameters are:</p> <ul> <li>Bounding Boxes (spatial query)</li> <li>A time interval (temporal query)</li> <li>Labels (semantic query)</li> </ul> Protocol BuffersFlatbuffers <p>Source: <code>examples/python/gRPC/images/gRPC_pb_queryImage.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nimport sys\nimport image_service_pb2_grpc as imageService\nimport meta_operations_pb2_grpc as metaOperations\nimport query_pb2 as query\nfrom google.protobuf import empty_pb2\n# importing util functions. Assuming that this file is in the parent dir\n# examples/python/gRPC/util.py\nscript_dir = os.path.dirname(__file__)\nutil_dir = os.path.join(script_dir, '..')\nsys.path.append(util_dir)\nimport util\n# Default server is localhost !\nchannel = util.get_gRPC_channel()\n# 1. Get gRPC service objects\nstub = imageService.ImageServiceStub(channel)\nstubMeta = metaOperations.MetaOperationsStub(channel)\n# 2. Get all projects from the server\nresponse = stubMeta.GetProjects(empty_pb2.Empty())\n# 3. Check if we have an existing test project, if not, we stop here\nprojectuuid = \"\"\nfor project in response.projects:\nprint(project.name + \" \" + project.uuid + \"\\n\")\nif project.name == \"testproject\":\nprojectuuid = project.uuid\nif projectuuid == \"\":\nsys.exit()\n# 4. Create a query with parameters\ntheQuery = query.Query()\ntheQuery.projectuuid.append(projectuuid)\ntheQuery.boundingboxstamped.header.frame_id = \"map\"\ntheQuery.boundingboxstamped.boundingbox.point_min.x = 0.0\ntheQuery.boundingboxstamped.boundingbox.point_min.y = 0.0\ntheQuery.boundingboxstamped.boundingbox.point_min.z = 0.0\ntheQuery.boundingboxstamped.boundingbox.point_max.x = 100.0\ntheQuery.boundingboxstamped.boundingbox.point_max.y = 100.0\ntheQuery.boundingboxstamped.boundingbox.point_max.z = 100.0\n# since epoche\ntheQuery.timeinterval.time_min.seconds = 1638549273\ntheQuery.timeinterval.time_min.nanos = 0\ntheQuery.timeinterval.time_max.seconds = 1938549273\ntheQuery.timeinterval.time_max.nanos = 0\n# labels\ntheQuery.label.extend([\"testlabel0\"])\n# 5. Query the server for images matching the query and iterate over them\nfor img in stub.GetImage(theQuery):\nprint(f\"uuidmsg: {img.header.uuid_msgs}\")\nprint(f\"first label: {img.labels_bb[0].labelWithInstance.label}\")\nprint(\n\"First bounding box (Xmin, Ymin, Xmax, Ymax): \"\n+ str(img.labels_bb[0].boundingBox.point_min.x)\n+ \" \"\n+ str(img.labels_bb[0].boundingBox.point_min.y)\n+ \" \"\n+ str(img.labels_bb[0].boundingBox.point_max.x)\n+ \" \"\n+ str(img.labels_bb[0].boundingBox.point_max.y)\n+ \"\\n\"\n)\n</code></pre> <p>Output:</p> <pre><code>testproject 3af70ba8-1e81-4f60-86d2-a4257d88f01e\n\nfirst label: testlabel0\nFirst bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04\n\nfirst label: testlabel0\nFirst bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04\n\nfirst label: testlabel0\nFirst bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04\n\nfirst label: testlabel0\nFirst bounding box (Xmin, Ymin, Xmax, Ymax): 0.01 0.02 0.03 0.04\n</code></pre> <p>Source: <code>examples/images/gRPC/images/gRPC_fb_queryImage.py</code></p> <pre><code>#!/usr/bin/env python3\nimport os\nimport sys\nimport flatbuffers\nfrom fb import Image\nfrom fb import image_service_grpc_fb as imageService\n# importing util functions. Assuming that these files are in the parent dir\n# examples/python/gRPC/util.py\n# examples/python/gRPC/util_fb.py\nscript_dir = os.path.dirname(__file__)\nutil_dir = os.path.join(script_dir, '..')\nsys.path.append(util_dir)\nimport util\nimport util_fb\nbuilder = flatbuffers.Builder(1024)\n# Default server is localhost !\nchannel = util.get_gRPC_channel()\n# 1. Get all projects from the server\nprojectuuid = util_fb.getProject(builder, channel, 'testproject')\n# 2. Check if the defined project exist; if not exit\nif not projectuuid:\nexit()\n# 3. Get gRPC service object\nstub = imageService.ImageServiceStub(channel)\n# Create all necessary objects for the query\nheader = util_fb.createHeader(builder, frame=\"map\")\npointMin = util_fb.createPoint(builder, 0.0, 0.0, 0.0)\npointMax = util_fb.createPoint(builder, 100.0, 100.0, 100.0)\nboundingboxStamped = util_fb.createBoundingBoxStamped(builder, header, pointMin, pointMax)\ntimeMin = util_fb.createTimeStamp(builder, 1610549273, 0)\ntimeMax = util_fb.createTimeStamp(builder, 1938549273, 0)\ntimeInterval = util_fb.createTimeInterval(builder, timeMin, timeMax)\nprojectUuids = [builder.CreateString(projectuuid)]\nlabels = [builder.CreateString(\"testlabel0\")]\ndataUuids = [builder.CreateString(\"3e12e18d-2d53-40bc-a8af-c5cca3c3b248\")]\ninstanceUuids = [builder.CreateString(\"3e12e18d-2d53-40bc-a8af-c5cca3c3b248\")]\n# 4. Create a query with parameters\n# all parameters are optional\n# with all parameters set (especially with the data and instance uuids set) the result\n# of the query will be empty. Set the query parameters to adequate values or remove\n# them from the query creation\nquery = util_fb.createQuery(\nbuilder,\n# boundingBox=boundingboxStamped,\n# timeInterval=timeInterval,\n# labels=labels,\n# projectUuids=projectUuids,\n# instanceUuids=instanceUuids,\n# dataUuids=dataUuids,\nwithoutData=True,\n)\nbuilder.Finish(query)\nbuf = builder.Output()\n# 5. Query the server for images matching the query and iterate over them\nfor responseBuf in stub.GetImage(bytes(buf)):\nresponse = Image.Image.GetRootAs(responseBuf)\nprint(f\"uuidmsg: {response.Header().UuidMsgs().decode('utf-8')}\")\nprint(\"first label: \" + response.LabelsBb(0).LabelWithInstance().Label().decode(\"utf-8\"))\nprint(\n\"first bounding box (Xmin,Ymin,Xmax,Ymax): \"\n+ str(response.LabelsBb(0).BoundingBox().PointMin().X())\n+ \" \"\n+ str(response.LabelsBb(0).BoundingBox().PointMin().Y())\n+ \" \"\n+ str(response.LabelsBb(0).BoundingBox().PointMax().X())\n+ \" \"\n+ str(response.LabelsBb(0).BoundingBox().PointMax().Y())\n+ \"\\n\"\n)\nprint(\"done.\")\n</code></pre> <p>Output:</p> <pre><code>testproject 3af70ba8-1e81-4f60-86d2-a4257d88f01e\n\nuuidmsg: 00ced216-40b1-4d54-817f-11c413b228c6\nfirst label: testlabel0\nfirst bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04\n\nuuidmsg: 5836f989-adbb-46a0-a689-9e0527d457fe\nfirst label: testlabel0\nfirst bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04\n\nuuidmsg: 5b39487d-238a-43e4-a8a4-0f7efb876e8b\nfirst label: testlabel0\nfirst bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04\n\nuuidmsg: 5d330208-e534-4bfb-b242-00295e6b027d\nfirst label: testlabel0\nfirst bounding box (Xmin,Ymin,Xmax,Ymax): 0.01 0.02 0.03 0.04\n\ndone.\n</code></pre>"},{"location":"tutorials/overview/","title":"Tutorials Overviews","text":"<p>The tutorials provide a starting point on how you can use SEEREP. Currently, the following topics are covered:</p> <ul> <li>Creating and Retrieving projects</li> <li>Sending and Querying images</li> </ul> <p>Before running any of the tutorials, make sure that you have a  running SEEREP instance available.</p>"},{"location":"tutorials/overview/#local-instance","title":"Local Instance","text":"<p>To start SEEREP locally use <code>STRG+SHIFT+D</code> to open the Run &amp; Debug Menu in Vs-Code, select <code>seerep server</code> and press run. Now a terminal should open and print the following info messages:</p> <pre><code>Starting seerep server\n[2022-08-01 13:50:35.765427]&lt;info&gt;: The used logging folder is: /seerep/seerep-data/log/\n[2022-08-01 13:50:35.765575]&lt;info&gt;: The used data folder is: /seerep/seerep-data/\n[2022-08-01 13:50:35.765801]&lt;info&gt;: add the protobuf gRPC services...\n[2022-08-01 13:50:35.765860]&lt;info&gt;: add the flatbuffer gRPC services...\n[2022-08-01 13:50:35.767787]&lt;info&gt;: serving gRPC Server on \"[::]:9090\"...\n</code></pre>"},{"location":"tutorials/projects/","title":"Creating &amp; Retrieving Projects","text":""},{"location":"tutorials/projects/#creating-new-projects","title":"Creating new projects","text":"<p>New projects for new data can be created in the following way:</p> <p>Source: <code>examples/gRPC/meta/gRPC_pb_createProject.py</code></p> <pre><code>import os\nimport sys\nimport meta_operations_pb2_grpc as metaOperations\nimport projectCreation_pb2\n# importing util functions. Assuming that this file is in the parent dir\n# examples/python/gRPC/util.py\nscript_dir = os.path.dirname(__file__)\nutil_dir = os.path.join(script_dir, '..')\nsys.path.append(util_dir)\nimport util\n# Default server is localhost !\nchannel = util.get_gRPC_channel()\n# 1. Get gRPC service object\nstub = metaOperations.MetaOperationsStub(channel)\n# 2. Create the project object inline and send it to the server\nresponse = stub.CreateProject(projectCreation_pb2.ProjectCreation(name=\"testproject\", mapFrameId=\"map\"))\nprint(\"The new project on the server is (name/uuid):\")\nprint(\"\\t\" + response.name + \" \" + response.uuid)\n</code></pre> <p>Output:</p> <pre><code>The new project on the server is (name/uuid):\n        testproject eff47bc9-c39e-430e-8153-88e0eab65768\n</code></pre>"},{"location":"tutorials/projects/#retrieving-projects","title":"Retrieving projects","text":"<p>After we created two projects, we can query them. Currently the name doesn't have to be unique.</p> Protocol BuffersFlatbuffers <p>Source: <code>examples/gRPC/meta/gRPC_pb_getProjects.py</code></p> <pre><code>import os\nimport sys\nimport meta_operations_pb2_grpc as metaOperations\nfrom google.protobuf import empty_pb2\n# importing util functions. Assuming that this file is in the parent dir\n# examples/python/gRPC/util.py\nscript_dir = os.path.dirname(__file__)\nutil_dir = os.path.join(script_dir, '..')\nsys.path.append(util_dir)\nimport util\n# Default server is localhost !\nchannel = util.get_gRPC_channel()\n# 1. Get gRPC service object\nstub = metaOperations.MetaOperationsStub(channel)\n# 2. Get all projects on the server\nresponse = stub.GetProjects(empty_pb2.Empty())\nprint(\"The server has the following projects (name/uuid):\")\nfor projectinfo in response.projects:\nprint(\"\\t\" + projectinfo.name + \" \" + projectinfo.uuid)\n</code></pre> <p>Output:</p> <pre><code>The Server has the following projects (name/uuid):\n    testproject 5c1ed18e-9180-40e1-a79b-594f8266d898\n    testproject 9fc3011f-4a3c-400e-9170-06973a6fb395\n</code></pre> <p>Source: <code>examples/gRPC/meta/gRPC_fb_getProjects.py</code></p> <pre><code>import os\nimport sys\nimport flatbuffers\nfrom fb import Empty, ProjectInfos\nfrom fb import meta_operations_grpc_fb as metaOperations\n# importing util functions. Assuming that this file is in the parent dir\n# examples/python/gRPC/util.py\nscript_dir = os.path.dirname(__file__)\nutil_dir = os.path.join(script_dir, '..')\nsys.path.append(util_dir)\nimport util\n# Default server is localhost !\nchannel = util.get_gRPC_channel()\n# 1. Get gRPC service object\nstub = metaOperations.MetaOperationsStub(channel)\n# Create an empty message\nbuilder = flatbuffers.Builder(1024)\nEmpty.Start(builder)\nemptyMsg = Empty.End(builder)\nbuilder.Finish(emptyMsg)\nbuf = builder.Output()\n# 2. Get all projects on the server\nresponseBuf = stub.GetProjects(bytes(buf))\nresponse = ProjectInfos.ProjectInfos.GetRootAs(responseBuf)\nprint(\"The server has the following projects (name/uuid):\")\nfor i in range(response.ProjectsLength()):\nprint(\"\\t\" + response.Projects(i).Name().decode(\"utf-8\") + \" \" + response.Projects(i).Uuid().decode(\"utf-8\"))\n</code></pre> <p>Output:</p> <pre><code>The server has the following projects (name/uuid):\n    testproject 5c1ed18e-9180-40e1-a79b-594f8266d898\n    testproject 9fc3011f-4a3c-400e-9170-06973a6fb395\n</code></pre>"}]}