{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home The objective of SEEREP (SEmantic Environment REPresentation) is to store generated robot data and enable fast spatio-temporal-semantic queries over the data. Context Autonomous robotic systems must be aware of their environment, in order to safely achieve their goal-oriented actions. Especially in unstructured and changing environments, a detailed model of the environment is required for reasoning and planning. The sensors of a robot provide spatial information via the robot's pose, temporal information is created by the point in time when a sensor is read. Semantic information always exists implicitly and can be made explicit by algorithms or manual labeling. Most existing environment representations focus on one or two of these information types, SEEREP is able to store all three. Thereby, SEEREP enables the robot is able to reason on a higher level and disambiguate sensor data based on the context. Core Features Fast spatio-temporal-semantic queries with gRPC . Storage of data generated by the robotic system: Offline on the robot (no or slow internet connection). Online on a server-cluster, with gRPC . Shifting computation loads away from the robot and into the cloud. Easily switch between Protocol Buffers (PB) / Flatbuffers (FB) as the messaging format. Architecture The following graphic provides a broad overview of the different components of SEEREP. The sensor data along with processing results and annotations are stored in HDF5 files. SEEREP uses projects to group common information (e.g. a scanning campaign). The data can be saved locally on the robot or sent into the cloud with gRPC. Due to that, the computation load on the robot can be reduced and algorithms can fetch the data subset which they actually need. A more detailed version, with all ROS packages and message types is available in the package overview . Publications (need to be added when published)","title":"Home"},{"location":"#home","text":"The objective of SEEREP (SEmantic Environment REPresentation) is to store generated robot data and enable fast spatio-temporal-semantic queries over the data.","title":"Home"},{"location":"#context","text":"Autonomous robotic systems must be aware of their environment, in order to safely achieve their goal-oriented actions. Especially in unstructured and changing environments, a detailed model of the environment is required for reasoning and planning. The sensors of a robot provide spatial information via the robot's pose, temporal information is created by the point in time when a sensor is read. Semantic information always exists implicitly and can be made explicit by algorithms or manual labeling. Most existing environment representations focus on one or two of these information types, SEEREP is able to store all three. Thereby, SEEREP enables the robot is able to reason on a higher level and disambiguate sensor data based on the context.","title":"Context"},{"location":"#core-features","text":"Fast spatio-temporal-semantic queries with gRPC . Storage of data generated by the robotic system: Offline on the robot (no or slow internet connection). Online on a server-cluster, with gRPC . Shifting computation loads away from the robot and into the cloud. Easily switch between Protocol Buffers (PB) / Flatbuffers (FB) as the messaging format.","title":"Core Features"},{"location":"#architecture","text":"The following graphic provides a broad overview of the different components of SEEREP. The sensor data along with processing results and annotations are stored in HDF5 files. SEEREP uses projects to group common information (e.g. a scanning campaign). The data can be saved locally on the robot or sent into the cloud with gRPC. Due to that, the computation load on the robot can be reduced and algorithms can fetch the data subset which they actually need. A more detailed version, with all ROS packages and message types is available in the package overview .","title":"Architecture"},{"location":"#publications","text":"(need to be added when published)","title":"Publications"},{"location":"data-protection-notice/","text":"Data Protection Notice The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data. In this data protection policy we inform you about the processing of your personal data when visiting and using our web site. Controller Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Phone: +49 631 20575 0 info@dfki.de Legal Notice Data protection officer Phone: +49 631 20575 0 datenschutz@dfki.de Hosting Server This website is hosted with Github . For more information about github's data processing and contacts, please refer to the Privacy Policy . Access and Intervention Besides the information in this data protection policy you have the right of access to your personal data. To ensure fair data processing, you have the following rights: The right to rectification and completion of your personal data The right to erasure of your personal data The right to restriction of the processing of your personal data The right to object to the processing of your personal data on grounds related to your particular situation To exercise these rights, please contact our data protection officer. Right to lodge a complaint You have the right to lodge a complaint with a supervisory authority if you consider that the processing of your personal data infringes statutory data protection regulations.","title":"Data Protection Notice"},{"location":"data-protection-notice/#data-protection-notice","text":"The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data. In this data protection policy we inform you about the processing of your personal data when visiting and using our web site.","title":"Data Protection Notice"},{"location":"data-protection-notice/#controller","text":"Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Phone: +49 631 20575 0 info@dfki.de Legal Notice","title":"Controller"},{"location":"data-protection-notice/#data-protection-officer","text":"Phone: +49 631 20575 0 datenschutz@dfki.de","title":"Data protection officer"},{"location":"data-protection-notice/#hosting-server","text":"This website is hosted with Github . For more information about github's data processing and contacts, please refer to the Privacy Policy .","title":"Hosting Server"},{"location":"data-protection-notice/#access-and-intervention","text":"Besides the information in this data protection policy you have the right of access to your personal data. To ensure fair data processing, you have the following rights: The right to rectification and completion of your personal data The right to erasure of your personal data The right to restriction of the processing of your personal data The right to object to the processing of your personal data on grounds related to your particular situation To exercise these rights, please contact our data protection officer.","title":"Access and Intervention"},{"location":"data-protection-notice/#right-to-lodge-a-complaint","text":"You have the right to lodge a complaint with a supervisory authority if you consider that the processing of your personal data infringes statutory data protection regulations.","title":"Right to lodge a complaint"},{"location":"docs/","text":"Docs The documentation is published via GitHub Pages. The responsible workflow combines Doxygen and MkDocs into one page, so it is more convenient to ues. MkDocs focusses on higher level concepts like the installation process and a package overview, while Doxygen ist used for code documentation. If you want to work on the documentation locally i.e for a PR follow the steps below. Locally the two documentation systems must be run individually. Dependencies If you are not using the SEEREP development container , you need to have doxygen and MkDocs-Material installed, use the commands below: pip3 mkdocs-material sudo apt install doxygen MkDocs To run MkDocs locally switch into the main directory of SEEREP, where the mkdocs.yml is located. Then use mkdocs serve to build and deploy MkDocs on a local http-server. The page should then be available under http://127.0.0.1:8000/ . Doxygen To create the Doxygen output locally switch into the main directory of SEEREP, where the Doxyfile is located and run doxygen Doxyfile . Now an html/ folder should be in the same directory. If you are not working in the development container you can simply open the index.html with your browser of choice. Otherwise, switch into the html folder and use python3 -m http.server to start a local web server who serves the content. The page should be available under the default http://0.0.0.0:8000/ address. If you want to run MkDocs and Doxygen at the same time you need to provide a different port, use python3 -m http.server 8002 instead.","title":"Docs"},{"location":"docs/#docs","text":"The documentation is published via GitHub Pages. The responsible workflow combines Doxygen and MkDocs into one page, so it is more convenient to ues. MkDocs focusses on higher level concepts like the installation process and a package overview, while Doxygen ist used for code documentation. If you want to work on the documentation locally i.e for a PR follow the steps below. Locally the two documentation systems must be run individually.","title":"Docs"},{"location":"docs/#dependencies","text":"If you are not using the SEEREP development container , you need to have doxygen and MkDocs-Material installed, use the commands below: pip3 mkdocs-material sudo apt install doxygen","title":"Dependencies"},{"location":"docs/#mkdocs","text":"To run MkDocs locally switch into the main directory of SEEREP, where the mkdocs.yml is located. Then use mkdocs serve to build and deploy MkDocs on a local http-server. The page should then be available under http://127.0.0.1:8000/ .","title":"MkDocs"},{"location":"docs/#doxygen","text":"To create the Doxygen output locally switch into the main directory of SEEREP, where the Doxyfile is located and run doxygen Doxyfile . Now an html/ folder should be in the same directory. If you are not working in the development container you can simply open the index.html with your browser of choice. Otherwise, switch into the html folder and use python3 -m http.server to start a local web server who serves the content. The page should be available under the default http://0.0.0.0:8000/ address. If you want to run MkDocs and Doxygen at the same time you need to provide a different port, use python3 -m http.server 8002 instead.","title":"Doxygen"},{"location":"installation/","text":"Installation This page provides an overview on ways to install SEEREP. VS Code Devcontainer Recommended way of installing SEEREP Requirement : Docker Version >= 17.12.0 Clone the SEEREP repository from Github and open it in VS-Code. git clone https://github.com/agri-gaia/seerep cd seerep/ && code Create a sibling folder to the repo folder called seerep-data . This folder will be mounted for the data exchange between host and container. Without this folder, the following steps will fail! mkdir ../seerep-data Install the Remote Containers and Docker extension. code --install-extension ms-vscode-remote.remote-containers code --install-extension ms-azuretools.vscode-docker Press F1 or CTRL + SHIFT + P in VS-Code and enter `Remote-Containers: Reopen Folder in Container This can take a couple of minutes since, a docker container based on the seerep/dev-image is downloaded and started. Additionally, all necessary VS Code extensions are installed and Intellisense / pre-commit hooks are set up. VS-Code may ask you to login, to get the latest updates from the repository. The current folder will be mounted in the docker container with default user: docker and password: docker . Pre Commit Checks Use pre-commit run -a in the workspace folder to check the code style before committing. The docker image already has the pre-commit checks installed, thus committing is only possible if the checks succeed. Remote Host Currently not tested ! If you want to use a devcontainer on a remote host, add the following to settings.json of your VS-Code config in your local workspace and replace the placeholders with your values: \"docker.host\": \"ssh://your-remote-user@your-remote-machine-fqdn-or-ip-here\", Also update the workspace mount to a volume in the devcontainer.json file: \"workspaceMount\": \"source=seerep-ws,target=/home/docker/workspace/src,type=volume\", Make sure that your SSH agent is running and that it knows the keys to connect to the remote host. Instructions on that can be found here . Manual Installation Install ROS Noetic with the offical documentation Install SEEREPs dependencies: gRPC Protocol Buffers Flatbuffers HighFive Therefore, please follow the installDependecies.sh script. Warning It is not recommended to install these dependencies globally. Some of them are really hard to uninstall !! Building SEEREP We provide two ways to build SEEREP: First with catkin build and second with cmake/make. Catkin Build source /opt/ros/$ROS_DISTRO/setup.bash mkdir -p seerep_ws/src cd seerep_ws/src git clone git@github.com:agri-gaia/seerep.git cd .. catkin build","title":"Installation"},{"location":"installation/#installation","text":"This page provides an overview on ways to install SEEREP.","title":"Installation"},{"location":"installation/#vs-code-devcontainer","text":"Recommended way of installing SEEREP Requirement : Docker Version >= 17.12.0 Clone the SEEREP repository from Github and open it in VS-Code. git clone https://github.com/agri-gaia/seerep cd seerep/ && code Create a sibling folder to the repo folder called seerep-data . This folder will be mounted for the data exchange between host and container. Without this folder, the following steps will fail! mkdir ../seerep-data Install the Remote Containers and Docker extension. code --install-extension ms-vscode-remote.remote-containers code --install-extension ms-azuretools.vscode-docker Press F1 or CTRL + SHIFT + P in VS-Code and enter `Remote-Containers: Reopen Folder in Container This can take a couple of minutes since, a docker container based on the seerep/dev-image is downloaded and started. Additionally, all necessary VS Code extensions are installed and Intellisense / pre-commit hooks are set up. VS-Code may ask you to login, to get the latest updates from the repository. The current folder will be mounted in the docker container with default user: docker and password: docker .","title":"VS Code Devcontainer"},{"location":"installation/#pre-commit-checks","text":"Use pre-commit run -a in the workspace folder to check the code style before committing. The docker image already has the pre-commit checks installed, thus committing is only possible if the checks succeed.","title":"Pre Commit Checks"},{"location":"installation/#remote-host","text":"Currently not tested ! If you want to use a devcontainer on a remote host, add the following to settings.json of your VS-Code config in your local workspace and replace the placeholders with your values: \"docker.host\": \"ssh://your-remote-user@your-remote-machine-fqdn-or-ip-here\", Also update the workspace mount to a volume in the devcontainer.json file: \"workspaceMount\": \"source=seerep-ws,target=/home/docker/workspace/src,type=volume\", Make sure that your SSH agent is running and that it knows the keys to connect to the remote host. Instructions on that can be found here .","title":"Remote Host"},{"location":"installation/#manual-installation","text":"Install ROS Noetic with the offical documentation Install SEEREPs dependencies: gRPC Protocol Buffers Flatbuffers HighFive Therefore, please follow the installDependecies.sh script. Warning It is not recommended to install these dependencies globally. Some of them are really hard to uninstall !!","title":"Manual Installation"},{"location":"installation/#building-seerep","text":"We provide two ways to build SEEREP: First with catkin build and second with cmake/make.","title":"Building SEEREP"},{"location":"installation/#catkin-build","text":"source /opt/ros/$ROS_DISTRO/setup.bash mkdir -p seerep_ws/src cd seerep_ws/src git clone git@github.com:agri-gaia/seerep.git cd .. catkin build","title":"Catkin Build"},{"location":"legal-notice/","text":"LEGAL NOTICE Responsible service provider Responsible for the content of the domain agri-gaia.github.io/seerep from the point of view of \u00a7 5 TMG: Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Kr\u00fcger Helmut Ditzer Trippstadter Str. 122 67663 Kaiserslautern Germany Phone: +49 631 20575 0 Fax: +49 631 20575 5030 Email: info@dfki.de Register Court: Amtsgericht Kaiserslautern Register Number: HRB 2313 ID-Number: DE 148 646 973 The person responsible for the editorial content of the domain agri-gaia.github.io/seerep of the German Research Center for Artificial Intelligence GmbH within the meaning of \u00a7 18 para. 2 MStV is: Mark H\u00f6llmann Berghoffstra\u00dfe 11, 49090 Osnabr\u00fcck Phone: +49 (0) 541 386050 2254 E-mail: mark.hoellmann@dfki.de Website URL: www.dfki.de Liability for content As a service provider, Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) is responsible under general law for its own content published on this website in accordance with Section 7 para. 1 of the German Telemedia Act (TMG). DFKI makes every effort to keep the information on our website accurate and current, nevertheless, errors and uncertainties cannot be entirely ruled out. For this reason, DFKI undertakes no liability for ensuring that the provided information is current, accurate or complete, and is not responsible for its quality. DFKI is not liable for material or immaterial damages caused directly or indirectly by the use or non-use of the offered information, or by the use of erroneous and incomplete information, unless willful or grossly negligent fault can be demonstrated. This also applies with respect to software or data provided for download. DFKI reserves the right to modify, expand or delete parts of the website or the entire website without separate announcement, or to cease publication temporarily or definitively. Liability for links Pursuant to Section 7 para. 1 of TMG (German Tele-Media Act), the law limits our responsibility as a service provider to our own content on this website. According to Sections 8 \u2013 10 TMG, we are not obliged to permanently monitor any transmitted or stored third-party information, nor to investigate circumstances that might indicate illegal activity. This does not affect our obligation to remove or block information according to general law. However, we can only assume liability for such data from the point in time at which a concrete legal infringement has been identified. Upon notification of any such legal infringement, we will immediately delete the infringing content. Cross-references (\u201clinks\u201d) to the content providers are to be distinguished from our own content. Our offer includes links to external third-party websites. Providers or operators of linked external pages are always responsible for their respective content. We cannot assume any liability for the content of the linked pages. This third-party content was checked by DFKI when the links were first set up to determine whether any legal infringements existed. At the time of the check, no legal infringements were apparent. However, it cannot be ruled out that the content is subsequently changed by the respective providers. A permanent control of the content of the linked pages is not reasonable without evidence of a legal infringement. Should you believe that the linked external pages infringe applicable law or otherwise contain inappropriate content, please notify us directly at: info@dfki.de. In case DFKI should notice or receive any indication that an external offer to which it has linked might cause civil or criminal liability, DFKI will immediately delete this link. Copyright The layout of the homepage, the graphics used and other content on the DFKI website are protected by copyright. The reproduction, processing, distribution and any type of use outside the boundaries of copyright law require the prior written approval of the DFKI. Insofar as any content on this page was not created by DFKI, the copyrights of third parties will be respected. If you believe that you discovered a copyright infringement, please report this to us accordingly. Upon becoming aware of any legal infringements, DFKI will remove or disable access to such infringing content immediately.","title":"LEGAL NOTICE"},{"location":"legal-notice/#legal-notice","text":"","title":"LEGAL NOTICE"},{"location":"legal-notice/#responsible-service-provider","text":"Responsible for the content of the domain agri-gaia.github.io/seerep from the point of view of \u00a7 5 TMG: Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Kr\u00fcger Helmut Ditzer Trippstadter Str. 122 67663 Kaiserslautern Germany Phone: +49 631 20575 0 Fax: +49 631 20575 5030 Email: info@dfki.de Register Court: Amtsgericht Kaiserslautern Register Number: HRB 2313 ID-Number: DE 148 646 973 The person responsible for the editorial content of the domain agri-gaia.github.io/seerep of the German Research Center for Artificial Intelligence GmbH within the meaning of \u00a7 18 para. 2 MStV is: Mark H\u00f6llmann Berghoffstra\u00dfe 11, 49090 Osnabr\u00fcck Phone: +49 (0) 541 386050 2254 E-mail: mark.hoellmann@dfki.de Website URL: www.dfki.de","title":"Responsible service provider"},{"location":"legal-notice/#liability-for-content","text":"As a service provider, Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH (DFKI) is responsible under general law for its own content published on this website in accordance with Section 7 para. 1 of the German Telemedia Act (TMG). DFKI makes every effort to keep the information on our website accurate and current, nevertheless, errors and uncertainties cannot be entirely ruled out. For this reason, DFKI undertakes no liability for ensuring that the provided information is current, accurate or complete, and is not responsible for its quality. DFKI is not liable for material or immaterial damages caused directly or indirectly by the use or non-use of the offered information, or by the use of erroneous and incomplete information, unless willful or grossly negligent fault can be demonstrated. This also applies with respect to software or data provided for download. DFKI reserves the right to modify, expand or delete parts of the website or the entire website without separate announcement, or to cease publication temporarily or definitively.","title":"Liability for content"},{"location":"legal-notice/#liability-for-links","text":"Pursuant to Section 7 para. 1 of TMG (German Tele-Media Act), the law limits our responsibility as a service provider to our own content on this website. According to Sections 8 \u2013 10 TMG, we are not obliged to permanently monitor any transmitted or stored third-party information, nor to investigate circumstances that might indicate illegal activity. This does not affect our obligation to remove or block information according to general law. However, we can only assume liability for such data from the point in time at which a concrete legal infringement has been identified. Upon notification of any such legal infringement, we will immediately delete the infringing content. Cross-references (\u201clinks\u201d) to the content providers are to be distinguished from our own content. Our offer includes links to external third-party websites. Providers or operators of linked external pages are always responsible for their respective content. We cannot assume any liability for the content of the linked pages. This third-party content was checked by DFKI when the links were first set up to determine whether any legal infringements existed. At the time of the check, no legal infringements were apparent. However, it cannot be ruled out that the content is subsequently changed by the respective providers. A permanent control of the content of the linked pages is not reasonable without evidence of a legal infringement. Should you believe that the linked external pages infringe applicable law or otherwise contain inappropriate content, please notify us directly at: info@dfki.de. In case DFKI should notice or receive any indication that an external offer to which it has linked might cause civil or criminal liability, DFKI will immediately delete this link.","title":"Liability for links"},{"location":"legal-notice/#copyright","text":"The layout of the homepage, the graphics used and other content on the DFKI website are protected by copyright. The reproduction, processing, distribution and any type of use outside the boundaries of copyright law require the prior written approval of the DFKI. Insofar as any content on this page was not created by DFKI, the copyrights of third parties will be respected. If you believe that you discovered a copyright infringement, please report this to us accordingly. Upon becoming aware of any legal infringements, DFKI will remove or disable access to such infringing content immediately.","title":"Copyright"},{"location":"packages/","text":"Package Overview This page provides an overview of all the ROS-Packages used in SEEREP. General SEEREP Structure The general structure of SEEREP is schematically illustrated in the following graphic. SEEREP can be split into two parts, one of which runs on the robot (left box) and one which runs on a server clusters (right box). The communication is handled via gRPC and protobuffers (PB) / flatbuffers (FB) . Packages In the following, each package will be described in more detail. seerep-hdf5 The seerep-hd5f unit provides access to the hdf5 files to store or retrieve data. The unit is split into three packages seeprep-hdf5-core , seerep-hdf5-pb and seerep-hdf5-fb . This is to have a server-core which is independent of the message format, so that it's possible to easily switch between PB and FB or any other message format. The main task for the seerep-core is to read UUIDs, bounding boxes (BB), time/semantic information on provided indices from the hdf5-files. The only write operation of the core is to create new hdf5-files. Due to the independence of FB and PB, new communication-messages are added to seerep-msgs ( seerep-msgs/core ). seerep-hdf5-pb and seerep-hdf5-fb provide methods to read or write pointclouds, images and transformations from PB or FB messages. seerep-srv The seerep-srv is also split into three parts seerep-server , seerep-core and seerep-core-pb . The seerep-server provides the top level interface for the SEEREP server cluster, services which clients can use are registered here. The server passes request to the corresponding unit in the layer below (see graphic). The seerep-core-pb writes incoming PB messages to the hdf5 files. In case of a query the seerep-core is asked for the UUIDs of the datasets which match the query parameters. This core could be easily switched to one that uses FB. seerep-msgs The seerep-msgs package defines all the PB, FB and core messages used in SEEREP. seerep-ros seerep-ros provides two packages which run on the robot itself. The first package seerep_ros_conversions simply converts ROS messages to PB messages and vice versa. The second package seerep_ros_communication is used to save sensor information like images and point clouds on the robot, or in case of a good internet connection to the remote server-cluster. Further, the robot is able to query the server for information to support his understanding of the environment and aid its navigation. The seerep_ros_communication\\client is responsible for sending sensor information direclty to the remote server. The seerep_ros_communication\\querier is used to get information from the remote server. seerep_ros_communicatio\\hdf5-dump is used to save sensor information on a hard drive which is located on the robot. seerep-com seerep-com is used to define the gRPC services in PB and FB.","title":"Package Overview"},{"location":"packages/#package-overview","text":"This page provides an overview of all the ROS-Packages used in SEEREP.","title":"Package Overview"},{"location":"packages/#general-seerep-structure","text":"The general structure of SEEREP is schematically illustrated in the following graphic. SEEREP can be split into two parts, one of which runs on the robot (left box) and one which runs on a server clusters (right box). The communication is handled via gRPC and protobuffers (PB) / flatbuffers (FB) .","title":"General SEEREP Structure"},{"location":"packages/#packages","text":"In the following, each package will be described in more detail.","title":"Packages"},{"location":"packages/#seerep-hdf5","text":"The seerep-hd5f unit provides access to the hdf5 files to store or retrieve data. The unit is split into three packages seeprep-hdf5-core , seerep-hdf5-pb and seerep-hdf5-fb . This is to have a server-core which is independent of the message format, so that it's possible to easily switch between PB and FB or any other message format. The main task for the seerep-core is to read UUIDs, bounding boxes (BB), time/semantic information on provided indices from the hdf5-files. The only write operation of the core is to create new hdf5-files. Due to the independence of FB and PB, new communication-messages are added to seerep-msgs ( seerep-msgs/core ). seerep-hdf5-pb and seerep-hdf5-fb provide methods to read or write pointclouds, images and transformations from PB or FB messages.","title":"seerep-hdf5"},{"location":"packages/#seerep-srv","text":"The seerep-srv is also split into three parts seerep-server , seerep-core and seerep-core-pb . The seerep-server provides the top level interface for the SEEREP server cluster, services which clients can use are registered here. The server passes request to the corresponding unit in the layer below (see graphic). The seerep-core-pb writes incoming PB messages to the hdf5 files. In case of a query the seerep-core is asked for the UUIDs of the datasets which match the query parameters. This core could be easily switched to one that uses FB.","title":"seerep-srv"},{"location":"packages/#seerep-msgs","text":"The seerep-msgs package defines all the PB, FB and core messages used in SEEREP.","title":"seerep-msgs"},{"location":"packages/#seerep-ros","text":"seerep-ros provides two packages which run on the robot itself. The first package seerep_ros_conversions simply converts ROS messages to PB messages and vice versa. The second package seerep_ros_communication is used to save sensor information like images and point clouds on the robot, or in case of a good internet connection to the remote server-cluster. Further, the robot is able to query the server for information to support his understanding of the environment and aid its navigation. The seerep_ros_communication\\client is responsible for sending sensor information direclty to the remote server. The seerep_ros_communication\\querier is used to get information from the remote server. seerep_ros_communicatio\\hdf5-dump is used to save sensor information on a hard drive which is located on the robot.","title":"seerep-ros"},{"location":"packages/#seerep-com","text":"seerep-com is used to define the gRPC services in PB and FB.","title":"seerep-com"},{"location":"testing/","text":"Tests We are currently working on integrating more tests into SEEREP. GoogleTest is used as a testing framework. The tests are also run in a GitHub workflow for every PR and push to the main branch, additionally the tests can be run locally in a couple of different ways. Running tests locally Catkin The tests can be run via catkin in the command line. When run without a specific package, all test in the workspace are run. But while this is very convenient, catkin does not provide much information/output if a test fails. catkin test ( <specific-package> ) Vs-Code Another way to run the test is via the Vs-Code test explorer (triangle test-tube on the left bar of VS-Code). If you have done a fresh installation of the project it can happen, that the test cases won't be recognized. In order to fix that, just restart the development container. For that you can use Reopen Folder Locally and then Reopen In Container . Now you should be able to see the test cases as, in the example: The icons in the top are mostly self-explanatory, all tests can be run, a single test can be debugged, and a terminal can be opened to print the output of the tests. Executables If you would like to run the tests via their executables, they are located under /seerep/devel/bin/<test-name> or /seerep/build/<package>/<test-name> .","title":"Tests"},{"location":"testing/#tests","text":"We are currently working on integrating more tests into SEEREP. GoogleTest is used as a testing framework. The tests are also run in a GitHub workflow for every PR and push to the main branch, additionally the tests can be run locally in a couple of different ways.","title":"Tests"},{"location":"testing/#running-tests-locally","text":"","title":"Running tests locally"},{"location":"testing/#catkin","text":"The tests can be run via catkin in the command line. When run without a specific package, all test in the workspace are run. But while this is very convenient, catkin does not provide much information/output if a test fails. catkin test ( <specific-package> )","title":"Catkin"},{"location":"testing/#vs-code","text":"Another way to run the test is via the Vs-Code test explorer (triangle test-tube on the left bar of VS-Code). If you have done a fresh installation of the project it can happen, that the test cases won't be recognized. In order to fix that, just restart the development container. For that you can use Reopen Folder Locally and then Reopen In Container . Now you should be able to see the test cases as, in the example: The icons in the top are mostly self-explanatory, all tests can be run, a single test can be debugged, and a terminal can be opened to print the output of the tests.","title":"Vs-Code"},{"location":"testing/#executables","text":"If you would like to run the tests via their executables, they are located under /seerep/devel/bin/<test-name> or /seerep/build/<package>/<test-name> .","title":"Executables"},{"location":"tutorials-c%2B%2B/","text":"Tutorials in C++","title":"C++"},{"location":"tutorials-c%2B%2B/#tutorials-in-c","text":"","title":"Tutorials in C++"},{"location":"tutorials-python/","text":"Tutorials in python This tutorial will give an introduction on how to use SEEREP. The following topics are covered: Creating and retrieving projects Sending and querying data with gRPC Sending ROS messages to SEEREP Before running any of the code exampels, make sure that the SEEREP server is running . GRPC GRPC is used to send data back and forth between the robot and the cloud sever-cluster. In the build process, the protoc compiler created python classes from the service and messages definitions in protocol buffer (.proto) files. The generated classes are now used in the examples, for more information on the process, visit the gRPC documentation . Creating new projects Import gRPC and necessary service and message modules. Create a server connection, since localhost is used, we don't need any encryption. Create a stub (client) which provides the methods from the remote server. Call the CreateProject method with a projectCreation message as the parameter. import grpc import meta_operations_pb2_grpc as metaOperations # project service import projectCreation_pb2 # project creation message server = \"localhost:9090\" channel = grpc . insecure_channel ( server ) stub = metaOperations . MetaOperationsStub ( channel ) # create new project response = stub . CreateProject ( projectCreation_pb2 . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" ) ) print ( f \"The new project on the server { server } is (name/uuid):\" ) print ( f \" \\t { response . name } { response . uuid } \" ) Retrieving new projects Retrieving projects follows almost the same steps as creating a project, except that the GetProjects method as no parameter and needs an empty message . import grpc import meta_operations_pb2_grpc as metaOperations # project service from google.protobuf import empty_pb2 # empty message server = \"localhost:9090\" channel = grpc . insecure_channel ( server ) stub = metaOperations . MetaOperationsStub ( channel ) response = stub . GetProjects ( empty_pb2 . Empty ()) print ( f \" { server } has the following projects (name/uuid):\" ) for projectinfo in response . projects : print ( f \" \\t { projectinfo . name } { projectinfo . uuid } \" ) Sending images Import gRPC and necessary service and message modules. Check if we have an existing project, if not create one. Create an image to send. Or transform an image into the necessary structure. Set meta information for the image. Write image to the server. import image_pb2 as image # image message import image_service_pb2_grpc as imageService # image service import projectCreation_pb2 as projectCreation # project creation message import meta_operations_pb2_grpc as metaOperations # project service from google.protobuf import empty_pb2 # empty message import grpc import time import numpy as np channel = grpc . insecure_channel ( \"localhost:9090\" ) stubImage = imageService . ImageServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # check if the test-project is already present found = False for project in response . projects : print ( f \" { project . name } { project . uuid } \" ) if project . name == \"testproject\" : projectuuid = project . uuid found = True # create a project if the test-project is not present if not found : creation = projectCreation . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" ) response = stubMeta . CreateProject ( creation ) projectuuid = response . uuid theImage = image . Image () theTime = time . time () # create 10 images for n in range ( 10 ): theImage = image . Image () rgb = [] lim = 256 # 256x256 image for i in range ( lim ): for j in range ( lim ): x = float ( i ) / lim y = float ( j ) / lim z = float ( j ) / lim r = np . ubyte (( x * 255.0 + n ) % 255 ) g = np . ubyte (( y * 255.0 + n ) % 255 ) b = np . ubyte (( z * 255.0 + n ) % 255 ) # print(r, g, b) rgb . append ( r ) rgb . append ( g ) rgb . append ( b ) theImage . header . frame_id = \"camera\" theImage . header . stamp . seconds = theTime + n theImage . header . stamp . nanos = 0 theImage . header . uuid_project = projectuuid theImage . height = lim theImage . width = lim theImage . encoding = \"rgb8\" theImage . step = 3 * lim theImage . data = bytes ( rgb ) # add labels to the images for i in range ( 0 , 10 ): theImage . labels_general . append ( \"testlabelgeneral\" + str ( i )) uuidImg = stub . TransferImage ( theImage ) print ( \"uuid of transfered img: \" + uuidImg . message ) Querry images Import gRPC and necessary service and message modules. Get the UUID of the project you want to query. Create a query and set the query parameters. Get back images which match the query parameters. import image_service_pb2_grpc as imageService # image service import meta_operations_pb2_grpc as metaOperations # project service import query_pb2 as query # query message from google.protobuf import empty_pb2 # empty message import grpc channel = grpc . insecure_channel ( \"localhost:9090\" ) stub = imageService . ImageServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # get uuid of the testproject projectuuid = \"\" for project in response . projects : print ( project . name + \" \" + project . uuid ) if project . name == \"testproject\" : projectuuid = project . uuid # if the test project doesn't exits stop if projectuuid == \"\" : sys . exit () # cerate and query and set query parameters theQuery = query . Query () theQuery . projectuuid = projectuuid # since epoche theQuery . timeinterval . time_min = 1638549273 theQuery . timeinterval . time_max = 1938549273 # labels theQuery . label . extend ([ \"testlabel0\" ]) for img in stub . GetImage ( theQuery ): print ( \"uuid of transfered img: \" + img . labels_general [ 0 ])","title":"Python"},{"location":"tutorials-python/#tutorials-in-python","text":"This tutorial will give an introduction on how to use SEEREP. The following topics are covered: Creating and retrieving projects Sending and querying data with gRPC Sending ROS messages to SEEREP Before running any of the code exampels, make sure that the SEEREP server is running .","title":"Tutorials in python"},{"location":"tutorials-python/#grpc","text":"GRPC is used to send data back and forth between the robot and the cloud sever-cluster. In the build process, the protoc compiler created python classes from the service and messages definitions in protocol buffer (.proto) files. The generated classes are now used in the examples, for more information on the process, visit the gRPC documentation .","title":"GRPC"},{"location":"tutorials-python/#creating-new-projects","text":"Import gRPC and necessary service and message modules. Create a server connection, since localhost is used, we don't need any encryption. Create a stub (client) which provides the methods from the remote server. Call the CreateProject method with a projectCreation message as the parameter. import grpc import meta_operations_pb2_grpc as metaOperations # project service import projectCreation_pb2 # project creation message server = \"localhost:9090\" channel = grpc . insecure_channel ( server ) stub = metaOperations . MetaOperationsStub ( channel ) # create new project response = stub . CreateProject ( projectCreation_pb2 . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" ) ) print ( f \"The new project on the server { server } is (name/uuid):\" ) print ( f \" \\t { response . name } { response . uuid } \" )","title":"Creating new projects"},{"location":"tutorials-python/#retrieving-new-projects","text":"Retrieving projects follows almost the same steps as creating a project, except that the GetProjects method as no parameter and needs an empty message . import grpc import meta_operations_pb2_grpc as metaOperations # project service from google.protobuf import empty_pb2 # empty message server = \"localhost:9090\" channel = grpc . insecure_channel ( server ) stub = metaOperations . MetaOperationsStub ( channel ) response = stub . GetProjects ( empty_pb2 . Empty ()) print ( f \" { server } has the following projects (name/uuid):\" ) for projectinfo in response . projects : print ( f \" \\t { projectinfo . name } { projectinfo . uuid } \" )","title":"Retrieving new projects"},{"location":"tutorials-python/#sending-images","text":"Import gRPC and necessary service and message modules. Check if we have an existing project, if not create one. Create an image to send. Or transform an image into the necessary structure. Set meta information for the image. Write image to the server. import image_pb2 as image # image message import image_service_pb2_grpc as imageService # image service import projectCreation_pb2 as projectCreation # project creation message import meta_operations_pb2_grpc as metaOperations # project service from google.protobuf import empty_pb2 # empty message import grpc import time import numpy as np channel = grpc . insecure_channel ( \"localhost:9090\" ) stubImage = imageService . ImageServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # check if the test-project is already present found = False for project in response . projects : print ( f \" { project . name } { project . uuid } \" ) if project . name == \"testproject\" : projectuuid = project . uuid found = True # create a project if the test-project is not present if not found : creation = projectCreation . ProjectCreation ( name = \"testproject\" , mapFrameId = \"map\" ) response = stubMeta . CreateProject ( creation ) projectuuid = response . uuid theImage = image . Image () theTime = time . time () # create 10 images for n in range ( 10 ): theImage = image . Image () rgb = [] lim = 256 # 256x256 image for i in range ( lim ): for j in range ( lim ): x = float ( i ) / lim y = float ( j ) / lim z = float ( j ) / lim r = np . ubyte (( x * 255.0 + n ) % 255 ) g = np . ubyte (( y * 255.0 + n ) % 255 ) b = np . ubyte (( z * 255.0 + n ) % 255 ) # print(r, g, b) rgb . append ( r ) rgb . append ( g ) rgb . append ( b ) theImage . header . frame_id = \"camera\" theImage . header . stamp . seconds = theTime + n theImage . header . stamp . nanos = 0 theImage . header . uuid_project = projectuuid theImage . height = lim theImage . width = lim theImage . encoding = \"rgb8\" theImage . step = 3 * lim theImage . data = bytes ( rgb ) # add labels to the images for i in range ( 0 , 10 ): theImage . labels_general . append ( \"testlabelgeneral\" + str ( i )) uuidImg = stub . TransferImage ( theImage ) print ( \"uuid of transfered img: \" + uuidImg . message )","title":"Sending images"},{"location":"tutorials-python/#querry-images","text":"Import gRPC and necessary service and message modules. Get the UUID of the project you want to query. Create a query and set the query parameters. Get back images which match the query parameters. import image_service_pb2_grpc as imageService # image service import meta_operations_pb2_grpc as metaOperations # project service import query_pb2 as query # query message from google.protobuf import empty_pb2 # empty message import grpc channel = grpc . insecure_channel ( \"localhost:9090\" ) stub = imageService . ImageServiceStub ( channel ) stubMeta = metaOperations . MetaOperationsStub ( channel ) response = stubMeta . GetProjects ( empty_pb2 . Empty ()) # get uuid of the testproject projectuuid = \"\" for project in response . projects : print ( project . name + \" \" + project . uuid ) if project . name == \"testproject\" : projectuuid = project . uuid # if the test project doesn't exits stop if projectuuid == \"\" : sys . exit () # cerate and query and set query parameters theQuery = query . Query () theQuery . projectuuid = projectuuid # since epoche theQuery . timeinterval . time_min = 1638549273 theQuery . timeinterval . time_max = 1938549273 # labels theQuery . label . extend ([ \"testlabel0\" ]) for img in stub . GetImage ( theQuery ): print ( \"uuid of transfered img: \" + img . labels_general [ 0 ])","title":"Querry images"}]}